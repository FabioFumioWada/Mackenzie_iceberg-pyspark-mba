{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dNQHqJs0mPv"
   },
   "source": [
    "# Trabalhando com Snapshots e Time Travel\n",
    "### Snapshots (Instantâneos)\n",
    "O Snapshot é uma \"fotografia\" imutável do estado do sistema de dados em um ponto específico no tempo.\n",
    "\n",
    "**O que é:** Uma cópia lógica e instantânea dos dados e metadados. Diferente de um backup tradicional que copia todos os dados, um snapshot frequentemente armazena apenas os ponteiros para os dados originais e registra as alterações subsequentes (técnica copy-on-write).\n",
    "\n",
    "**Propósito:** Serve como um ponto de restauração rápido e eficiente. Permite que o sistema retorne a um estado conhecido e consistente em caso de erro, corrupção de dados ou falha.\n",
    "\n",
    "**Característica Chave:** É um registro do estado em um momento exato.\n",
    "\n",
    "### Time Travel (Viagem no Tempo)\n",
    "O Time Travel é a capacidade de consultar ou restaurar dados de qualquer um dos Snapshots disponíveis em um período de retenção predefinido.\n",
    "\n",
    "**O que é:** Um mecanismo que aproveita os Snapshots (ou o histórico de alterações) para permitir que os usuários executem consultas em versões antigas dos dados, como se estivessem \"viajando no tempo\".\n",
    "\n",
    "**Propósito:**\n",
    "\n",
    "**Análise:** Consultar dados históricos para relatórios de fim de período ou auditoria.\n",
    "\n",
    "**Recuperação Granular:** Restaurar apenas uma tabela, um esquema, ou até mesmo dados excluídos/modificados acidentalmente, sem precisar restaurar o banco de dados inteiro.\n",
    "\n",
    "**Debugging:** Rastrear a evolução dos dados e entender quando e como uma modificação específica ocorreu.\n",
    "\n",
    "**Característica Chave:** É a funcionalidade que usa os Snapshots para acessar o histórico de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Configuração do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3tZdZJzd7N8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.3.0-bin-hadoop3\"\n",
    "\n",
    "import findspark\n",
    "findspark.init('/opt/spark-3.3.0-bin-hadoop3')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergWithSpark\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", \"/home/tavares/warehouse\") \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26lDhrdl2KaM"
   },
   "source": [
    "2. Criando a tabela de vendas usando Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hadoop_catalog.default.vendas (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Inserir dados usando SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas VALUES\n",
    "    (1, 'Produto A', 10, 15.5, DATE('2023-11-01')),\n",
    "    (2, 'Produto B', 5, 22.0, DATE('2023-11-02')),\n",
    "    (3, 'Produto C', 8, 30.0, DATE('2023-11-03'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Visualização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2023-11-01|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Lista de snapshots atuais da tabela vendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|5100972200523424556|2025-11-02 19:23:33.734|append   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT snapshot_id, committed_at, operation FROM hadoop_catalog.default.vendas.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Incluimos mais dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas VALUES\n",
    "    (4, 'Produto D', 12, 25.0, DATE('2023-11-04')),\n",
    "    (5, 'Produto E', 7, 18.5, DATE('2023-11-05'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Visualização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2023-11-01|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "|  4|Produto D|        12| 25.0|2023-11-04|\n",
      "|  5|Produto E|         7| 18.5|2023-11-05|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas order by id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Agora temos dois snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|5100972200523424556|2025-11-02 19:23:33.734|append   |\n",
      "|7295434045239260225|2025-11-02 19:23:41.229|append   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT snapshot_id, committed_at, operation FROM hadoop_catalog.default.vendas.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Vamos fazer uma atualização nos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    UPDATE hadoop_catalog.default.vendas \n",
    "    SET preco = preco * 1.1 \n",
    "    WHERE id IN (1, 2)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Visualizar dados após update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+------------------+----------+\n",
      "| id|  produto|quantidade|             preco|data_venda|\n",
      "+---+---------+----------+------------------+----------+\n",
      "|  1|Produto A|        10|             17.05|2023-11-01|\n",
      "|  2|Produto B|         5|24.200000000000003|2023-11-02|\n",
      "|  3|Produto C|         8|              30.0|2023-11-03|\n",
      "|  4|Produto D|        12|              25.0|2023-11-04|\n",
      "|  5|Produto E|         7|              18.5|2023-11-05|\n",
      "+---+---------+----------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas order by id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Ver todos os snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|5100972200523424556|2025-11-02 19:23:33.734|append   |\n",
      "|7295434045239260225|2025-11-02 19:23:41.229|append   |\n",
      "|655517064564772829 |2025-11-02 19:23:51.988|overwrite|\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT snapshot_id, committed_at, operation FROM hadoop_catalog.default.vendas.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Time Travel - consultar dados de um snapshot específico\n",
    "- Primeiro, vamos pegar o ID do primeiro snapshot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiro snapshot ID: 5100972200523424556\n",
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2023-11-01|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshots = spark.sql(\"SELECT snapshot_id FROM hadoop_catalog.default.vendas.snapshots ORDER BY committed_at\")\n",
    "first_snapshot = snapshots.collect()[0][0]\n",
    "print(f\"Primeiro snapshot ID: {first_snapshot}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consultar dados do primeiro snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM hadoop_catalog.default.vendas \n",
    "    VERSION AS OF {first_snapshot}\n",
    "    ORDER BY id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Histórico de mudanças na tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Informações sobre arquivos da tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT file_path, file_format, record_count FROM hadoop_catalog.default.vendas.files\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
