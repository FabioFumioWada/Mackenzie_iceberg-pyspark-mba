{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dNQHqJs0mPv"
   },
   "source": [
    "# Uso de Metadados e Catálogo no Apache Iceberg\n",
    "\n",
    "Este notebook demonstra como utilizar os ricos metadados e recursos de catálogo do Apache Iceberg para monitoramento, análise e governança de dados.\n",
    "\n",
    "## Contexto e Objetivos:\n",
    "\n",
    "### Importância dos Metadados:\n",
    "- **Governança**: Controle e rastreabilidade de dados\n",
    "- **Monitoramento**: Acompanhamento de performance e uso\n",
    "- **Otimização**: Identificação de oportunidades de melhoria\n",
    "- **Auditoria**: Compliance e histórico de mudanças\n",
    "- **Troubleshooting**: Diagnóstico de problemas\n",
    "\n",
    "### Recursos de Metadados do Iceberg:\n",
    "- **Snapshots**: Histórico de versões da tabela\n",
    "- **Manifests**: Informações sobre arquivos de dados\n",
    "- **Files**: Detalhes de cada arquivo individual\n",
    "- **History**: Cronologia de operações\n",
    "- **Partitions**: Informações de particionamento\n",
    "- **Refs**: Referências e branches\n",
    "\n",
    "### Casos de Uso:\n",
    "1. **Monitoramento de Performance**: Identificar gargalos\n",
    "2. **Análise de Crescimento**: Acompanhar evolução dos dados\n",
    "3. **Otimização de Queries**: Entender padrões de acesso\n",
    "4. **Manutenção Preventiva**: Identificar necessidade de compactação\n",
    "5. **Auditoria de Dados**: Rastrear mudanças e acessos\n",
    "\n",
    "## Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-3tZdZJzd7N8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse configurado para: file:///home/tavares/warehouse\n"
     ]
    }
   ],
   "source": [
    "# Para o Spark se estiver rodando\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Configuração do Spark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.3.0-bin-hadoop3\"\n",
    "\n",
    "import findspark\n",
    "findspark.init('/opt/spark-3.3.0-bin-hadoop3')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergMetadata\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", \"file:///home/tavares/warehouse\") \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Warehouse configurado para: {spark.conf.get('spark.sql.catalog.hadoop_catalog.warehouse')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZrbavgI1b9Z"
   },
   "source": [
    "## 1. Criação de Tabela de Exemplo\n",
    "\n",
    "Vamos criar uma tabela que será usada para demonstrar os recursos de metadados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tmPQCmoeeHsj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando tabela de vendas\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.vendas\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE hadoop_catalog.default.vendas (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inserção de Dados para Gerar Metadados\n",
    "\n",
    "Vamos inserir dados em várias operações para criar um histórico rico de metadados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G0773bqNegO8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeira inserção realizada\n"
     ]
    }
   ],
   "source": [
    "# Primeira inserção de dados\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas VALUES\n",
    "    (1, 'Produto A', 10, 15.5, DATE('2024-01-15')),\n",
    "    (2, 'Produto B', 5, 22.0, DATE('2024-01-16')),\n",
    "    (3, 'Produto C', 8, 30.0, DATE('2024-01-17'))\n",
    "\"\"\")\n",
    "\n",
    "print(\"Primeira inserção realizada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segunda inserção realizada\n"
     ]
    }
   ],
   "source": [
    "# Segunda inserção de dados\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas VALUES\n",
    "    (4, 'Produto D', 12, 25.0, DATE('2024-02-18')),\n",
    "    (5, 'Produto E', 7, 18.5, DATE('2024-02-19'))\n",
    "\"\"\")\n",
    "\n",
    "print(\"Segunda inserção realizada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualização realizada\n"
     ]
    }
   ],
   "source": [
    "# Atualização de dados\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE hadoop_catalog.default.vendas \n",
    "    SET preco = preco * 1.1 \n",
    "    WHERE id <= 2\n",
    "\"\"\")\n",
    "\n",
    "print(\"Atualização realizada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explorando Metadados de Snapshots\n",
    "\n",
    "Os snapshots são fundamentais no Iceberg. Vamos explorar as informações disponíveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SNAPSHOTS DA TABELA ===\n",
      "+-------------------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|snapshot_id        |committed_at           |operation|summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-------------------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|3956159667302237607|2025-11-02 23:50:26.548|append   |{spark.app.id -> local-1762127418753, added-data-files -> 3, added-records -> 3, added-files-size -> 4177, changed-partition-count -> 1, total-records -> 3, total-files-size -> 4177, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762127418753, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}                                                                           |\n",
      "|5556644385757808960|2025-11-02 23:50:29.171|append   |{spark.app.id -> local-1762127418753, added-data-files -> 2, added-records -> 2, added-files-size -> 2786, changed-partition-count -> 1, total-records -> 5, total-files-size -> 6963, total-data-files -> 5, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762127418753, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}                                                                           |\n",
      "|8599238022227158035|2025-11-02 23:50:35.171|overwrite|{spark.app.id -> local-1762127418753, added-data-files -> 2, deleted-data-files -> 2, added-records -> 2, deleted-records -> 2, added-files-size -> 2849, removed-files-size -> 2785, changed-partition-count -> 1, total-records -> 5, total-files-size -> 7027, total-data-files -> 5, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762127418753, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}|\n",
      "+-------------------+-----------------------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "Total de snapshots: 3\n"
     ]
    }
   ],
   "source": [
    "# Informações básicas dos snapshots\n",
    "print(\"=== SNAPSHOTS DA TABELA ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        snapshot_id,\n",
    "        committed_at,\n",
    "        operation,\n",
    "        summary\n",
    "    FROM hadoop_catalog.default.vendas.snapshots \n",
    "    ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Contagem de snapshots\n",
    "snapshot_count = spark.sql(\"SELECT COUNT(*) as total FROM hadoop_catalog.default.vendas.snapshots\").collect()[0][0]\n",
    "print(f\"\\nTotal de snapshots: {snapshot_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análise do Histórico da Tabela\n",
    "\n",
    "O histórico fornece uma visão cronológica de todas as operações realizadas na tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HISTÓRICO DA TABELA ===\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2025-11-02 23:50:26.548|3956159667302237607|null               |true               |\n",
      "|2025-11-02 23:50:29.171|5556644385757808960|3956159667302237607|true               |\n",
      "|2025-11-02 23:50:35.171|8599238022227158035|5556644385757808960|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "=== ANÁLISE TEMPORAL ===\n",
      "+-------------+-------------+--------------------+--------------------+\n",
      "|data_operacao|num_operacoes|   primeira_operacao|     ultima_operacao|\n",
      "+-------------+-------------+--------------------+--------------------+\n",
      "|   2025-11-02|            3|2025-11-02 23:50:...|2025-11-02 23:50:...|\n",
      "+-------------+-------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Histórico completo da tabela\n",
    "print(\"=== HISTÓRICO DA TABELA ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        made_current_at,\n",
    "        snapshot_id,\n",
    "        parent_id,\n",
    "        is_current_ancestor\n",
    "    FROM hadoop_catalog.default.vendas.history \n",
    "    ORDER BY made_current_at\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Análise temporal\n",
    "print(\"\\n=== ANÁLISE TEMPORAL ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE(made_current_at) as data_operacao,\n",
    "        COUNT(*) as num_operacoes,\n",
    "        MIN(made_current_at) as primeira_operacao,\n",
    "        MAX(made_current_at) as ultima_operacao\n",
    "    FROM hadoop_catalog.default.vendas.history \n",
    "    GROUP BY DATE(made_current_at)\n",
    "    ORDER BY data_operacao\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Informações Detalhadas dos Arquivos\n",
    "\n",
    "Vamos analisar os arquivos de dados e suas características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ARQUIVOS DE DADOS ===\n",
      "+-------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+------------+---------------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|file_path                                                                                                    |file_format|record_count|file_size_in_bytes|file_size_kb|column_sizes                                 |value_counts                            |null_value_counts                       |\n",
      "+-------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+------------+---------------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00015-7-f5f70203-8385-4de5-b62f-6d7688b1cb82-0-00001.parquet|PARQUET    |1           |1425              |1.39        |{1 -> 42, 2 -> 51, 3 -> 42, 4 -> 46, 5 -> 42}|{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1, 5 -> 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00147-8-f5f70203-8385-4de5-b62f-6d7688b1cb82-0-00001.parquet|PARQUET    |1           |1424              |1.39        |{1 -> 42, 2 -> 50, 3 -> 42, 4 -> 46, 5 -> 42}|{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1, 5 -> 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00000-3-829e2257-74dd-4c96-8255-e5079f02d70d-0-00001.parquet|PARQUET    |1           |1393              |1.36        |{1 -> 36, 2 -> 45, 3 -> 36, 4 -> 40, 5 -> 42}|{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1, 5 -> 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00001-4-829e2257-74dd-4c96-8255-e5079f02d70d-0-00001.parquet|PARQUET    |1           |1393              |1.36        |{1 -> 36, 2 -> 45, 3 -> 36, 4 -> 40, 5 -> 42}|{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1, 5 -> 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00002-2-d6c712d4-b993-4b2f-a0e7-35ed0d5fd1e4-0-00001.parquet|PARQUET    |1           |1392              |1.36        |{1 -> 35, 2 -> 45, 3 -> 36, 4 -> 40, 5 -> 42}|{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1, 5 -> 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|\n",
      "+-------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+------------+---------------------------------------------+----------------------------------------+----------------------------------------+\n",
      "\n",
      "\n",
      "=== ESTATÍSTICAS DOS ARQUIVOS ===\n",
      "+--------------+---------------+---------------------------+-------------+-------------+-------------------+-------------------+\n",
      "|total_arquivos|total_registros|media_registros_por_arquivo|min_registros|max_registros|tamanho_total_bytes|tamanho_medio_bytes|\n",
      "+--------------+---------------+---------------------------+-------------+-------------+-------------------+-------------------+\n",
      "|             5|              5|                        1.0|            1|            1|               7027|             1405.4|\n",
      "+--------------+---------------+---------------------------+-------------+-------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Informações dos arquivos de dados\n",
    "print(\"=== ARQUIVOS DE DADOS ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        file_path,\n",
    "        file_format,\n",
    "        record_count,\n",
    "        file_size_in_bytes,\n",
    "        ROUND(file_size_in_bytes / 1024.0, 2) as file_size_kb,\n",
    "        column_sizes,\n",
    "        value_counts,\n",
    "        null_value_counts\n",
    "    FROM hadoop_catalog.default.vendas.files\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Estatísticas agregadas dos arquivos\n",
    "print(\"\\n=== ESTATÍSTICAS DOS ARQUIVOS ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_arquivos,\n",
    "        SUM(record_count) as total_registros,\n",
    "        AVG(record_count) as media_registros_por_arquivo,\n",
    "        MIN(record_count) as min_registros,\n",
    "        MAX(record_count) as max_registros,\n",
    "        SUM(file_size_in_bytes) as tamanho_total_bytes,\n",
    "        ROUND(AVG(file_size_in_bytes), 2) as tamanho_medio_bytes\n",
    "    FROM hadoop_catalog.default.vendas.files\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análise de Manifests\n",
    "\n",
    "Os manifests contêm metadados sobre os arquivos de dados e são essenciais para a performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MANIFESTS ===\n",
      "+-------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+\n",
      "|path                                                                                             |length|partition_spec_id|added_snapshot_id  |added_data_files_count|existing_data_files_count|deleted_data_files_count|\n",
      "+-------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+\n",
      "|file:/home/tavares/warehouse/default/vendas/metadata/7c25a193-a7c6-483f-8190-0814dd4cf0b9-m1.avro|6945  |0                |8599238022227158035|2                     |0                        |0                       |\n",
      "|file:/home/tavares/warehouse/default/vendas/metadata/6b49ef22-e9d4-48fe-8fd8-e9f496f0d34c-m0.avro|6928  |0                |5556644385757808960|2                     |0                        |0                       |\n",
      "|file:/home/tavares/warehouse/default/vendas/metadata/7c25a193-a7c6-483f-8190-0814dd4cf0b9-m0.avro|6981  |0                |8599238022227158035|0                     |1                        |2                       |\n",
      "+-------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+\n",
      "\n",
      "\n",
      "=== ESTATÍSTICAS DOS MANIFESTS ===\n",
      "+---------------+--------------------------+-------------------------+------------------------+----------------------+\n",
      "|total_manifests|total_arquivos_adicionados|total_arquivos_existentes|total_arquivos_deletados|tamanho_medio_manifest|\n",
      "+---------------+--------------------------+-------------------------+------------------------+----------------------+\n",
      "|              3|                         4|                        1|                       2|     6951.333333333333|\n",
      "+---------------+--------------------------+-------------------------+------------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Informações dos manifests\n",
    "print(\"=== MANIFESTS ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        path,\n",
    "        length,\n",
    "        partition_spec_id,\n",
    "        added_snapshot_id,\n",
    "        added_data_files_count,\n",
    "        existing_data_files_count,\n",
    "        deleted_data_files_count\n",
    "    FROM hadoop_catalog.default.vendas.manifests\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Estatísticas dos manifests\n",
    "print(\"\\n=== ESTATÍSTICAS DOS MANIFESTS ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_manifests,\n",
    "        SUM(added_data_files_count) as total_arquivos_adicionados,\n",
    "        SUM(existing_data_files_count) as total_arquivos_existentes,\n",
    "        SUM(deleted_data_files_count) as total_arquivos_deletados,\n",
    "        AVG(length) as tamanho_medio_manifest\n",
    "    FROM hadoop_catalog.default.vendas.manifests\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Informações do Schema e Particionamento\n",
    "\n",
    "Vamos explorar informações sobre o schema da tabela e particionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SCHEMA DA TABELA ===\n",
      "+----------------------------+---------------------------------------------+-------+\n",
      "|col_name                    |data_type                                    |comment|\n",
      "+----------------------------+---------------------------------------------+-------+\n",
      "|id                          |int                                          |       |\n",
      "|produto                     |string                                       |       |\n",
      "|quantidade                  |int                                          |       |\n",
      "|preco                       |double                                       |       |\n",
      "|data_venda                  |date                                         |       |\n",
      "|                            |                                             |       |\n",
      "|# Partitioning              |                                             |       |\n",
      "|Not partitioned             |                                             |       |\n",
      "|                            |                                             |       |\n",
      "|# Metadata Columns          |                                             |       |\n",
      "|_spec_id                    |int                                          |       |\n",
      "|_partition                  |struct<>                                     |       |\n",
      "|_file                       |string                                       |       |\n",
      "|_pos                        |bigint                                       |       |\n",
      "|_deleted                    |boolean                                      |       |\n",
      "|                            |                                             |       |\n",
      "|# Detailed Table Information|                                             |       |\n",
      "|Name                        |hadoop_catalog.default.vendas                |       |\n",
      "|Location                    |file:///home/tavares/warehouse/default/vendas|       |\n",
      "|Provider                    |iceberg                                      |       |\n",
      "+----------------------------+---------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "=== INFORMAÇÕES DE PARTICIONAMENTO ===\n",
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|           5|         5|                         7027|                           0|                         0|                           0|                         0|2025-11-02 23:50:...|     8599238022227158035|\n",
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n",
      "\n",
      "=== PROPRIEDADES DA TABELA ===\n",
      "+-------------------------------+-------------------+\n",
      "|key                            |value              |\n",
      "+-------------------------------+-------------------+\n",
      "|current-snapshot-id            |8599238022227158035|\n",
      "|format                         |iceberg/parquet    |\n",
      "|format-version                 |2                  |\n",
      "|write.parquet.compression-codec|zstd               |\n",
      "+-------------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema da tabela\n",
    "print(\"=== SCHEMA DA TABELA ===\")\n",
    "spark.sql(\"DESCRIBE EXTENDED hadoop_catalog.default.vendas\").show(truncate=False)\n",
    "\n",
    "# Informações de particionamento (se houver)\n",
    "print(\"\\n=== INFORMAÇÕES DE PARTICIONAMENTO ===\")\n",
    "try:\n",
    "    spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.partitions\").show()\n",
    "except:\n",
    "    print(\"Tabela não está particionada\")\n",
    "\n",
    "# Propriedades da tabela\n",
    "print(\"\\n=== PROPRIEDADES DA TABELA ===\")\n",
    "spark.sql(\"SHOW TBLPROPERTIES hadoop_catalog.default.vendas\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Análise de Performance e Otimização\n",
    "\n",
    "Usando metadados para identificar oportunidades de otimização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANÁLISE DE FRAGMENTAÇÃO ===\n",
      "+-------------------+------------+---------------+---------------+----------------+\n",
      "|  categoria_tamanho|num_arquivos|total_registros|media_registros|tamanho_total_mb|\n",
      "+-------------------+------------+---------------+---------------+----------------+\n",
      "|Muito Pequeno (<1K)|           5|              5|            1.0|            0.01|\n",
      "+-------------------+------------+---------------+---------------+----------------+\n",
      "\n",
      "\n",
      "=== RECOMENDAÇÕES ===\n",
      "⚠️  Arquivos pequenos (média: 1 registros): Compactação recomendada\n"
     ]
    }
   ],
   "source": [
    "# Análise de fragmentação de arquivos\n",
    "print(\"=== ANÁLISE DE FRAGMENTAÇÃO ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN record_count < 1000 THEN 'Muito Pequeno (<1K)'\n",
    "            WHEN record_count < 10000 THEN 'Pequeno (1K-10K)'\n",
    "            WHEN record_count < 100000 THEN 'Médio (10K-100K)'\n",
    "            ELSE 'Grande (>100K)'\n",
    "        END as categoria_tamanho,\n",
    "        COUNT(*) as num_arquivos,\n",
    "        SUM(record_count) as total_registros,\n",
    "        ROUND(AVG(record_count), 0) as media_registros,\n",
    "        ROUND(SUM(file_size_in_bytes) / 1024.0 / 1024.0, 2) as tamanho_total_mb\n",
    "    FROM hadoop_catalog.default.vendas.files\n",
    "    GROUP BY \n",
    "        CASE \n",
    "            WHEN record_count < 1000 THEN 'Muito Pequeno (<1K)'\n",
    "            WHEN record_count < 10000 THEN 'Pequeno (1K-10K)'\n",
    "            WHEN record_count < 100000 THEN 'Médio (10K-100K)'\n",
    "            ELSE 'Grande (>100K)'\n",
    "        END\n",
    "    ORDER BY categoria_tamanho\n",
    "\"\"\").show()\n",
    "\n",
    "# Recomendações baseadas em metadados\n",
    "file_count = spark.sql(\"SELECT COUNT(*) FROM hadoop_catalog.default.vendas.files\").collect()[0][0]\n",
    "avg_records = spark.sql(\"SELECT AVG(record_count) FROM hadoop_catalog.default.vendas.files\").collect()[0][0]\n",
    "\n",
    "print(f\"\\n=== RECOMENDAÇÕES ===\")\n",
    "if file_count > 10:\n",
    "    print(f\"⚠️  Muitos arquivos ({file_count}): Considere compactação\")\n",
    "if avg_records < 1000:\n",
    "    print(f\"⚠️  Arquivos pequenos (média: {avg_records:.0f} registros): Compactação recomendada\")\n",
    "if file_count <= 5 and avg_records > 1000:\n",
    "    print(\"✅ Estrutura de arquivos otimizada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monitoramento de Crescimento de Dados\n",
    "\n",
    "Análise temporal do crescimento dos dados usando metadados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CRESCIMENTO DE DADOS ===\n",
      "+-----------------------+---------+---------------------+-------------------+-------------------+\n",
      "|committed_at           |operation|registros_adicionados|registros_deletados|particoes_alteradas|\n",
      "+-----------------------+---------+---------------------+-------------------+-------------------+\n",
      "|2025-11-02 23:50:26.548|append   |3                    |0                  |1                  |\n",
      "|2025-11-02 23:50:29.171|append   |2                    |0                  |1                  |\n",
      "|2025-11-02 23:50:35.171|overwrite|2                    |2                  |1                  |\n",
      "+-----------------------+---------+---------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "=== EVOLUÇÃO DO TAMANHO ===\n",
      "+---------------+----------+\n",
      "|total_registros|tamanho_mb|\n",
      "+---------------+----------+\n",
      "|              5|      0.01|\n",
      "+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crescimento de dados ao longo do tempo\n",
    "print(\"=== CRESCIMENTO DE DADOS ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.committed_at,\n",
    "        s.operation,\n",
    "        COALESCE(s.summary['added-records'], '0') as registros_adicionados,\n",
    "        COALESCE(s.summary['deleted-records'], '0') as registros_deletados,\n",
    "        COALESCE(s.summary['changed-partition-count'], '0') as particoes_alteradas\n",
    "    FROM hadoop_catalog.default.vendas.snapshots s\n",
    "    ORDER BY s.committed_at\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Evolução do tamanho da tabela\n",
    "print(\"\\n=== EVOLUÇÃO DO TAMANHO ===\")\n",
    "current_size = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_registros,\n",
    "        ROUND(SUM(file_size_in_bytes) / 1024.0 / 1024.0, 2) as tamanho_mb\n",
    "    FROM hadoop_catalog.default.vendas.files\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Consultas Avançadas de Metadados\n",
    "\n",
    "Exemplos de consultas mais complexas para análise avançada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EFICIÊNCIA DE OPERAÇÕES ===\n",
      "+---------+-------------+----------------------+\n",
      "|operation|num_operacoes|media_registros_por_op|\n",
      "+---------+-------------+----------------------+\n",
      "|   append|            2|                   2.5|\n",
      "|overwrite|            1|                   2.0|\n",
      "+---------+-------------+----------------------+\n",
      "\n",
      "\n",
      "=== ARQUIVOS COM CARACTERÍSTICAS ESPECIAIS ===\n",
      "+-------------------------------------------------------------------------------------------------------------+------------+------------------+----------------------------------------+\n",
      "|file_path                                                                                                    |record_count|bytes_por_registro|null_value_counts                       |\n",
      "+-------------------------------------------------------------------------------------------------------------+------------+------------------+----------------------------------------+\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00015-7-f5f70203-8385-4de5-b62f-6d7688b1cb82-0-00001.parquet|1           |1425.0            |{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00147-8-f5f70203-8385-4de5-b62f-6d7688b1cb82-0-00001.parquet|1           |1424.0            |{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00000-3-829e2257-74dd-4c96-8255-e5079f02d70d-0-00001.parquet|1           |1393.0            |{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00001-4-829e2257-74dd-4c96-8255-e5079f02d70d-0-00001.parquet|1           |1393.0            |{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00002-2-d6c712d4-b993-4b2f-a0e7-35ed0d5fd1e4-0-00001.parquet|1           |1392.0            |{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|\n",
      "+-------------------------------------------------------------------------------------------------------------+------------+------------------+----------------------------------------+\n",
      "\n",
      "\n",
      "=== DENSIDADE DE DADOS ===\n",
      "+------------------------+----------------------+----------------------+\n",
      "|bytes_medio_por_registro|min_bytes_por_registro|max_bytes_por_registro|\n",
      "+------------------------+----------------------+----------------------+\n",
      "|                  1405.4|                1392.0|                1425.0|\n",
      "+------------------------+----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Análise de eficiência de operações\n",
    "print(\"=== EFICIÊNCIA DE OPERAÇÕES ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        operation,\n",
    "        COUNT(*) as num_operacoes,\n",
    "        AVG(CAST(COALESCE(summary['added-records'], '0') AS INT)) as media_registros_por_op\n",
    "    FROM hadoop_catalog.default.vendas.snapshots\n",
    "    GROUP BY operation\n",
    "    ORDER BY num_operacoes DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Identificar arquivos com estatísticas interessantes\n",
    "print(\"\\n=== ARQUIVOS COM CARACTERÍSTICAS ESPECIAIS ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        file_path,\n",
    "        record_count,\n",
    "        ROUND(file_size_in_bytes / record_count, 2) as bytes_por_registro,\n",
    "        null_value_counts\n",
    "    FROM hadoop_catalog.default.vendas.files\n",
    "    WHERE record_count > 0\n",
    "    ORDER BY bytes_por_registro DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Análise de densidade de dados\n",
    "print(\"\\n=== DENSIDADE DE DADOS ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ROUND(AVG(file_size_in_bytes / record_count), 2) as bytes_medio_por_registro,\n",
    "        ROUND(MIN(file_size_in_bytes / record_count), 2) as min_bytes_por_registro,\n",
    "        ROUND(MAX(file_size_in_bytes / record_count), 2) as max_bytes_por_registro\n",
    "    FROM hadoop_catalog.default.vendas.files\n",
    "    WHERE record_count > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo dos Recursos de Metadados\n",
    "\n",
    "### Tabelas de Metadados Disponíveis:\n",
    "\n",
    "1. **`table.snapshots`**: Histórico de versões da tabela\n",
    "   - snapshot_id, committed_at, operation, summary\n",
    "   - Útil para: auditoria, rollback, análise temporal\n",
    "\n",
    "2. **`table.history`**: Cronologia de mudanças\n",
    "   - made_current_at, snapshot_id, parent_id\n",
    "   - Útil para: rastreamento de mudanças, genealogia\n",
    "\n",
    "3. **`table.files`**: Informações detalhadas dos arquivos\n",
    "   - file_path, record_count, file_size_in_bytes, column_sizes\n",
    "   - Útil para: otimização, compactação, análise de performance\n",
    "\n",
    "4. **`table.manifests`**: Metadados dos manifests\n",
    "   - path, length, added_data_files_count\n",
    "   - Útil para: diagnóstico de performance, otimização de metadados\n",
    "\n",
    "5. **`table.partitions`**: Informações de particionamento\n",
    "   - partition, record_count, file_count\n",
    "   - Útil para: análise de distribuição, otimização de queries\n",
    "\n",
    "### Casos de Uso Práticos:\n",
    "\n",
    "- **Monitoramento**: Dashboards de crescimento e uso\n",
    "- **Otimização**: Identificação de necessidade de compactação\n",
    "- **Troubleshooting**: Diagnóstico de problemas de performance\n",
    "- **Governança**: Auditoria e compliance\n",
    "- **Planejamento**: Previsão de crescimento e recursos\n",
    "\n",
    "### Benefícios dos Metadados Ricos:\n",
    "\n",
    "- **Visibilidade**: Transparência completa sobre os dados\n",
    "- **Automação**: Possibilidade de automação baseada em métricas\n",
    "- **Otimização**: Decisões informadas sobre manutenção\n",
    "- **Confiabilidade**: Maior confiança na qualidade dos dados\n",
    "- **Eficiência**: Operações mais eficientes baseadas em dados reais"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNR5DDJ98pjkAvr9uwdOAth",
   "provenance": [
    {
     "file_id": "1NynsLfy1R-f8yQsXcpO1G7gPWRzUXxRI",
     "timestamp": 1731248613539
    },
    {
     "file_id": "1RbjXXITM0ttvuoaFrNaUZwtefAupJ1rc",
     "timestamp": 1731245177251
    },
    {
     "file_id": "178ZW6B8jLzISq4ceO5QdkIi2ZG-ybMAp",
     "timestamp": 1730934676588
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
