{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dNQHqJs0mPv"
   },
   "source": [
    "# Incorporando Dados Existentes ao Apache Iceberg\n",
    "\n",
    "Este notebook demonstra como migrar dados existentes em formato Parquet para tabelas Apache Iceberg. Esta é uma necessidade comum em projetos de modernização de data lakes, onde organizações já possuem dados em formatos tradicionais e desejam aproveitar os benefícios avançados do Iceberg.\n",
    "\n",
    "## Contexto e Objetivos:\n",
    "\n",
    "### Cenário Comum:\n",
    "- Dados históricos armazenados em formato Parquet\n",
    "- Necessidade de migrar para Iceberg sem perder dados\n",
    "- Manter compatibilidade durante a transição\n",
    "- Aproveitar recursos como ACID, time travel e schema evolution\n",
    "\n",
    "### Benefícios da Migração:\n",
    "- **ACID Transactions**: Garantia de consistência em operações\n",
    "- **Schema Evolution**: Evolução de esquema sem reescrita\n",
    "- **Time Travel**: Consultas históricas e rollbacks\n",
    "- **Partition Evolution**: Mudança de estratégias de particionamento\n",
    "- **Metadata Management**: Gerenciamento eficiente de metadados\n",
    "\n",
    "### Estratégias de Migração:\n",
    "1. **Migração Direta**: Leitura de Parquet e escrita em Iceberg\n",
    "2. **Migração Incremental**: Processamento em lotes menores\n",
    "3. **Migração com Transformação**: Aplicação de regras durante a migração\n",
    "\n",
    "## Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-3tZdZJzd7N8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse configurado para: file:///home/tavares/warehouse\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Sessão Spark\n",
    "# Para o Spark se estiver rodando\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergRollbacks\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", \"file:///home/tavares/warehouse\") \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Warehouse configurado para: {spark.conf.get('spark.sql.catalog.hadoop_catalog.warehouse')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwPKShjxtJlk"
   },
   "source": [
    "## 1. Criação da Tabela Iceberg de Destino\n",
    "\n",
    "Primeiro, criamos uma tabela Iceberg vazia com o mesmo esquema dos dados Parquet existentes. Esta tabela servirá como destino para a migração dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dCDuxVt3uMYJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar uma tabela Iceberg de destino\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.vendas_iceberg\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE hadoop_catalog.default.vendas_iceberg (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Leitura e Migração dos Dados Parquet\n",
    "\n",
    "Agora vamos ler os dados existentes em formato Parquet e migrá-los para a tabela Iceberg. Esta operação preserva todos os dados enquanto os converte para o formato Iceberg com seus benefícios adicionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e1DBtcu1-77E"
   },
   "outputs": [],
   "source": [
    "# le parquet\n",
    "df_parquet = spark.read.parquet(\"data/vendas_iceberg/\")\n",
    "# atualiza na tabela\n",
    "df_parquet.writeTo(\"hadoop_catalog.default.vendas_iceberg\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verificação dos Dados Migrados\n",
    "\n",
    "Após a migração, é importante verificar se todos os dados foram transferidos corretamente e se mantêm sua integridade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmHGPZJX_Vj6"
   },
   "outputs": [],
   "source": [
    "#visualizamos os dados\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_iceberg order by id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verificação dos Snapshots Criados\n",
    "\n",
    "Uma das vantagens imediatas da migração para Iceberg é o sistema de snapshots. Vamos verificar o snapshot criado durante a migração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9N-SjKCmuMpR"
   },
   "outputs": [],
   "source": [
    "# mostra snapshots\n",
    "snapshots_df = spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_iceberg.snapshots\")\n",
    "snapshots_df.select(\"snapshot_id\", \"committed_at\", \"operation\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMhOnuNNzAcBD330bu+rsmn",
   "provenance": [
    {
     "file_id": "178ZW6B8jLzISq4ceO5QdkIi2ZG-ybMAp",
     "timestamp": 1730935056622
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
