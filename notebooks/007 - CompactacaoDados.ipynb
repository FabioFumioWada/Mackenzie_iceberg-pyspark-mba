{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dNQHqJs0mPv"
   },
   "source": [
    "# Compacta√ß√£o de Dados com Apache Iceberg\n",
    "\n",
    "Este notebook demonstra como realizar compacta√ß√£o de dados no Apache Iceberg, uma opera√ß√£o essencial para manter performance e otimizar o armazenamento em data lakes.\n",
    "\n",
    "## Contexto e Objetivos:\n",
    "\n",
    "### Problema dos Arquivos Pequenos:\n",
    "- **Fragmenta√ß√£o**: M√∫ltiplas inser√ß√µes criam muitos arquivos pequenos\n",
    "- **Performance**: Overhead de metadados e I/O ineficiente\n",
    "- **Custos**: Maior n√∫mero de opera√ß√µes de leitura\n",
    "- **Escalabilidade**: Degrada√ß√£o com crescimento dos dados\n",
    "\n",
    "### Benef√≠cios da Compacta√ß√£o:\n",
    "- **Performance**: Menos arquivos = menos overhead de I/O\n",
    "- **Efici√™ncia**: Melhor compress√£o e utiliza√ß√£o de recursos\n",
    "- **Manuten√ß√£o**: Redu√ß√£o de metadados e complexidade\n",
    "- **Custos**: Otimiza√ß√£o de armazenamento e processamento\n",
    "\n",
    "### Estrat√©gias de Compacta√ß√£o:\n",
    "1. **Rewrite Data Files**: Reorganiza√ß√£o completa dos arquivos\n",
    "2. **Bin Packing**: Agrupamento eficiente de arquivos pequenos\n",
    "3. **Z-Order**: Otimiza√ß√£o para queries com m√∫ltiplas dimens√µes\n",
    "4. **Expire Snapshots**: Limpeza de vers√µes antigas\n",
    "\n",
    "## Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-3tZdZJzd7N8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse configurado para: file:///home/tavares/warehouse\n"
     ]
    }
   ],
   "source": [
    "# Para o Spark se estiver rodando\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Configura√ß√£o do Spark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.3.0-bin-hadoop3\"\n",
    "\n",
    "import findspark\n",
    "findspark.init('/opt/spark-3.3.0-bin-hadoop3')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Sess√£o Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergCompaction\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", \"file:///home/tavares/warehouse\") \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Warehouse configurado para: {spark.conf.get('spark.sql.catalog.hadoop_catalog.warehouse')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQiqFlRIRv8M"
   },
   "source": [
    "## 1. Cria√ß√£o da Tabela de Exemplo\n",
    "\n",
    "Vamos criar uma tabela que ser√° usada para demonstrar os conceitos de compacta√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K4KxJ5c2R0IS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exclui se existir\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.vendas\")\n",
    "\n",
    "# Cria a tabela Vendas\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE hadoop_catalog.default.vendas (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simula√ß√£o do Problema - Criando Arquivos Pequenos\n",
    "\n",
    "Vamos simular um cen√°rio comum onde m√∫ltiplas inser√ß√µes pequenas criam fragmenta√ß√£o de arquivos. Cada inser√ß√£o individual criar√° um arquivo separado, demonstrando o problema que a compacta√ß√£o resolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2JoI0LceWRjt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 inser√ß√µes individuais realizadas - cada uma criou um arquivo separado\n"
     ]
    }
   ],
   "source": [
    "# Inserir dados em lotes pequenos para criar v√°rios arquivos\n",
    "# Simulando inser√ß√µes frequentes de sistemas transacionais\n",
    "\n",
    "# 10 Lotes de Dados (cada um criar√° um arquivo separado)\n",
    "for i in range(1, 11):\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO hadoop_catalog.default.vendas VALUES\n",
    "        ({i}, 'Produto {i}', {i * 2}, {i * 10.0}, DATE('2024-11-{i:02d}'))\n",
    "    \"\"\")\n",
    "\n",
    "print(\"10 inser√ß√µes individuais realizadas - cada uma criou um arquivo separado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An√°lise do Estado Antes da Compacta√ß√£o\n",
    "\n",
    "Vamos analisar quantos arquivos foram criados e como os dados est√£o distribu√≠dos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dMxbIlnmWXUu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANTES DA COMPACTA√á√ÉO ===\n",
      "Distribui√ß√£o de registros por arquivo:\n",
      "+---------+-------------------------------------------------------------------------------------------------------------+\n",
      "|registros|arquivo                                                                                                      |\n",
      "+---------+-------------------------------------------------------------------------------------------------------------+\n",
      "|1        |file:/home/tavares/warehouse/default/vendas/data/00000-3-baf6144c-28f4-42d3-a539-b15ca2ef849e-0-00001.parquet|\n",
      "|1        |file:/home/tavares/warehouse/default/vendas/data/00000-9-ed871109-12af-460e-a995-0cb706b055aa-0-00001.parquet|\n",
      "|1        |file:/home/tavares/warehouse/default/vendas/data/00000-7-61eacd63-16fd-423c-a00d-30b039dc3c86-0-00001.parquet|\n",
      "|1        |file:/home/tavares/warehouse/default/vendas/data/00000-1-47ecbacb-9403-4475-85db-92b80dcedd3c-0-00001.parquet|\n",
      "|1        |file:/home/tavares/warehouse/default/vendas/data/00000-0-6acfa4a5-c43d-485f-9001-2e431b0da094-0-00001.parquet|\n",
      "|1        |file:/home/tavares/warehouse/default/vendas/data/00000-2-18cb0dc8-7683-4a5d-bead-e967d9e32da2-0-00001.parquet|\n",
      "|1        |file:/home/tavares/warehouse/default/vendas/data/00000-6-3621d48f-c6e4-4245-9b11-642d5818a004-0-00001.parquet|\n",
      "|1        |file:/home/tavares/warehouse/default/vendas/data/00000-5-610f7e9b-3e3f-4519-bfa3-ccdbde4fc387-0-00001.parquet|\n",
      "|1        |file:/home/tavares/warehouse/default/vendas/data/00000-8-6f4881ed-cbb5-4c3b-b1be-279540810609-0-00001.parquet|\n",
      "|1        |file:/home/tavares/warehouse/default/vendas/data/00000-4-feafc243-9942-4579-9516-a9157da35c04-0-00001.parquet|\n",
      "+---------+-------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "Resumo:\n",
      "- N√∫mero total de arquivos: 10\n",
      "- Total de registros: 10\n",
      "- M√©dia de registros por arquivo: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Analisar arquivos antes da compacta√ß√£o\n",
    "print(\"=== ANTES DA COMPACTA√á√ÉO ===\")\n",
    "\n",
    "# Contar arquivos √∫nicos\n",
    "df_files_before = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) AS registros,\n",
    "        input_file_name() AS arquivo\n",
    "    FROM hadoop_catalog.default.vendas\n",
    "    GROUP BY input_file_name()\n",
    "\"\"\")\n",
    "\n",
    "print(\"Distribui√ß√£o de registros por arquivo:\")\n",
    "df_files_before.show(truncate=False)\n",
    "\n",
    "num_arquivos_antes = df_files_before.count()\n",
    "total_registros = spark.sql(\"SELECT COUNT(*) as total FROM hadoop_catalog.default.vendas\").collect()[0][0]\n",
    "\n",
    "print(f\"\\nResumo:\")\n",
    "print(f\"- N√∫mero total de arquivos: {num_arquivos_antes}\")\n",
    "print(f\"- Total de registros: {total_registros}\")\n",
    "print(f\"- M√©dia de registros por arquivo: {total_registros / num_arquivos_antes:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verifica√ß√£o de Informa√ß√µes dos Arquivos\n",
    "\n",
    "Vamos usar as tabelas de metadados do Iceberg para obter informa√ß√µes detalhadas sobre os arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INFORMA√á√ïES DOS ARQUIVOS (METADADOS ICEBERG) ===\n",
      "+-------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+------------+\n",
      "|file_path                                                                                                    |file_format|record_count|file_size_in_bytes|file_size_kb|\n",
      "+-------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+------------+\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00000-0-6acfa4a5-c43d-485f-9001-2e431b0da094-0-00001.parquet|PARQUET    |1           |1393              |1.36        |\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00000-1-47ecbacb-9403-4475-85db-92b80dcedd3c-0-00001.parquet|PARQUET    |1           |1393              |1.36        |\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00000-2-18cb0dc8-7683-4a5d-bead-e967d9e32da2-0-00001.parquet|PARQUET    |1           |1392              |1.36        |\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00000-3-baf6144c-28f4-42d3-a539-b15ca2ef849e-0-00001.parquet|PARQUET    |1           |1392              |1.36        |\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00000-4-feafc243-9942-4579-9516-a9157da35c04-0-00001.parquet|PARQUET    |1           |1393              |1.36        |\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00000-5-610f7e9b-3e3f-4519-bfa3-ccdbde4fc387-0-00001.parquet|PARQUET    |1           |1393              |1.36        |\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00000-6-3621d48f-c6e4-4245-9b11-642d5818a004-0-00001.parquet|PARQUET    |1           |1393              |1.36        |\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00000-7-61eacd63-16fd-423c-a00d-30b039dc3c86-0-00001.parquet|PARQUET    |1           |1393              |1.36        |\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00000-8-6f4881ed-cbb5-4c3b-b1be-279540810609-0-00001.parquet|PARQUET    |1           |1393              |1.36        |\n",
      "|file:/home/tavares/warehouse/default/vendas/data/00000-9-ed871109-12af-460e-a995-0cb706b055aa-0-00001.parquet|PARQUET    |1           |1399              |1.37        |\n",
      "+-------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+------------+\n",
      "\n",
      "+-----------+-------------+--------------------+----------------+-------------------+\n",
      "|total_files|total_records|avg_records_per_file|total_size_bytes|avg_file_size_bytes|\n",
      "+-----------+-------------+--------------------+----------------+-------------------+\n",
      "|         10|           10|                 1.0|           13934|             1393.4|\n",
      "+-----------+-------------+--------------------+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Informa√ß√µes detalhadas dos arquivos usando metadados Iceberg\n",
    "print(\"=== INFORMA√á√ïES DOS ARQUIVOS (METADADOS ICEBERG) ===\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        file_path,\n",
    "        file_format,\n",
    "        record_count,\n",
    "        file_size_in_bytes,\n",
    "        ROUND(file_size_in_bytes / 1024.0, 2) as file_size_kb\n",
    "    FROM hadoop_catalog.default.vendas.files\n",
    "    ORDER BY file_path\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Estat√≠sticas agregadas\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_files,\n",
    "        SUM(record_count) as total_records,\n",
    "        AVG(record_count) as avg_records_per_file,\n",
    "        SUM(file_size_in_bytes) as total_size_bytes,\n",
    "        AVG(file_size_in_bytes) as avg_file_size_bytes\n",
    "    FROM hadoop_catalog.default.vendas.files\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Executando a Compacta√ß√£o\n",
    "\n",
    "Agora vamos executar a compacta√ß√£o usando o procedimento `rewrite_data_files` do Iceberg. Esta opera√ß√£o reorganizar√° os arquivos pequenos em arquivos maiores e mais eficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "R5PZ9Ql1Wbr-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando compacta√ß√£o...\n",
      "\n",
      "Resultado da compacta√ß√£o:\n",
      "+--------------------------+----------------------+---------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|\n",
      "+--------------------------+----------------------+---------------------+\n",
      "|                        10|                     1|                13934|\n",
      "+--------------------------+----------------------+---------------------+\n",
      "\n",
      "Compacta√ß√£o conclu√≠da!\n"
     ]
    }
   ],
   "source": [
    "# Configurar tamanho m√°ximo de registros por arquivo\n",
    "spark.conf.set(\"spark.sql.files.maxRecordsPerFile\", 1000)\n",
    "\n",
    "print(\"Executando compacta√ß√£o...\")\n",
    "\n",
    "# Compacta√ß√£o com procedimento 'rewrite_data_files'\n",
    "result = spark.sql(\"\"\"\n",
    "    CALL hadoop_catalog.system.rewrite_data_files(\n",
    "        table => 'default.vendas'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Mostrar resultado da compacta√ß√£o\n",
    "print(\"\\nResultado da compacta√ß√£o:\")\n",
    "result.show()\n",
    "\n",
    "print(\"Compacta√ß√£o conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. An√°lise do Estado Ap√≥s a Compacta√ß√£o\n",
    "\n",
    "Vamos verificar como a compacta√ß√£o afetou a estrutura dos arquivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6xUN7rOSWRnE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AP√ìS A COMPACTA√á√ÉO ===\n",
      "Nova distribui√ß√£o de registros por arquivo:\n",
      "+---------+--------------------------------------------------------------------------------------------------------------+\n",
      "|registros|arquivo                                                                                                       |\n",
      "+---------+--------------------------------------------------------------------------------------------------------------+\n",
      "|10       |file:/home/tavares/warehouse/default/vendas/data/00000-19-24e176cf-8bbd-40b1-b4df-35fc392418d6-0-00001.parquet|\n",
      "+---------+--------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "Resumo ap√≥s compacta√ß√£o:\n",
      "- N√∫mero total de arquivos: 1\n",
      "- Total de registros: 10\n",
      "- M√©dia de registros por arquivo: 10.0\n",
      "\n",
      "Melhoria:\n",
      "- Redu√ß√£o de arquivos: 10 ‚Üí 1 (90.0% redu√ß√£o)\n",
      "- Dados preservados: True\n"
     ]
    }
   ],
   "source": [
    "# Analisar arquivos ap√≥s a compacta√ß√£o\n",
    "print(\"=== AP√ìS A COMPACTA√á√ÉO ===\")\n",
    "\n",
    "df_files_after = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) AS registros,\n",
    "        input_file_name() AS arquivo\n",
    "    FROM hadoop_catalog.default.vendas\n",
    "    GROUP BY input_file_name()\n",
    "\"\"\")\n",
    "\n",
    "print(\"Nova distribui√ß√£o de registros por arquivo:\")\n",
    "df_files_after.show(truncate=False)\n",
    "\n",
    "num_arquivos_depois = df_files_after.count()\n",
    "total_registros_depois = spark.sql(\"SELECT COUNT(*) as total FROM hadoop_catalog.default.vendas\").collect()[0][0]\n",
    "\n",
    "print(f\"\\nResumo ap√≥s compacta√ß√£o:\")\n",
    "print(f\"- N√∫mero total de arquivos: {num_arquivos_depois}\")\n",
    "print(f\"- Total de registros: {total_registros_depois}\")\n",
    "print(f\"- M√©dia de registros por arquivo: {total_registros_depois / num_arquivos_depois:.1f}\")\n",
    "\n",
    "print(f\"\\nMelhoria:\")\n",
    "print(f\"- Redu√ß√£o de arquivos: {num_arquivos_antes} ‚Üí {num_arquivos_depois} ({((num_arquivos_antes - num_arquivos_depois) / num_arquivos_antes * 100):.1f}% redu√ß√£o)\")\n",
    "print(f\"- Dados preservados: {total_registros == total_registros_depois}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verifica√ß√£o dos Snapshots\n",
    "\n",
    "A compacta√ß√£o cria um novo snapshot. Vamos verificar o hist√≥rico de snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HIST√ìRICO DE SNAPSHOTS ===\n",
      "+-------------------+-----------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|snapshot_id        |committed_at           |operation|summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-------------------+-----------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|6730901837367380239|2025-11-06 23:57:30.827|append   |{spark.app.id -> local-1762473435997, added-data-files -> 1, added-records -> 1, added-files-size -> 1393, changed-partition-count -> 1, total-records -> 1, total-files-size -> 1393, total-data-files -> 1, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762473435997, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}                                           |\n",
      "|8332996358247609723|2025-11-06 23:57:31.144|append   |{spark.app.id -> local-1762473435997, added-data-files -> 1, added-records -> 1, added-files-size -> 1393, changed-partition-count -> 1, total-records -> 2, total-files-size -> 2786, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762473435997, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}                                           |\n",
      "|7004162128840245634|2025-11-06 23:57:31.391|append   |{spark.app.id -> local-1762473435997, added-data-files -> 1, added-records -> 1, added-files-size -> 1392, changed-partition-count -> 1, total-records -> 3, total-files-size -> 4178, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762473435997, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}                                           |\n",
      "|7824859467824520240|2025-11-06 23:57:31.58 |append   |{spark.app.id -> local-1762473435997, added-data-files -> 1, added-records -> 1, added-files-size -> 1392, changed-partition-count -> 1, total-records -> 4, total-files-size -> 5570, total-data-files -> 4, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762473435997, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}                                           |\n",
      "|3146811541287673173|2025-11-06 23:57:31.848|append   |{spark.app.id -> local-1762473435997, added-data-files -> 1, added-records -> 1, added-files-size -> 1393, changed-partition-count -> 1, total-records -> 5, total-files-size -> 6963, total-data-files -> 5, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762473435997, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}                                           |\n",
      "|1557981862336165702|2025-11-06 23:57:32.035|append   |{spark.app.id -> local-1762473435997, added-data-files -> 1, added-records -> 1, added-files-size -> 1393, changed-partition-count -> 1, total-records -> 6, total-files-size -> 8356, total-data-files -> 6, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762473435997, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}                                           |\n",
      "|1619525338135577957|2025-11-06 23:57:32.22 |append   |{spark.app.id -> local-1762473435997, added-data-files -> 1, added-records -> 1, added-files-size -> 1393, changed-partition-count -> 1, total-records -> 7, total-files-size -> 9749, total-data-files -> 7, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762473435997, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}                                           |\n",
      "|7100358377131212126|2025-11-06 23:57:32.481|append   |{spark.app.id -> local-1762473435997, added-data-files -> 1, added-records -> 1, added-files-size -> 1393, changed-partition-count -> 1, total-records -> 8, total-files-size -> 11142, total-data-files -> 8, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762473435997, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}                                          |\n",
      "|6116861660554806607|2025-11-06 23:57:32.657|append   |{spark.app.id -> local-1762473435997, added-data-files -> 1, added-records -> 1, added-files-size -> 1393, changed-partition-count -> 1, total-records -> 9, total-files-size -> 12535, total-data-files -> 9, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762473435997, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}                                          |\n",
      "|9020866284305659372|2025-11-06 23:57:32.793|append   |{spark.app.id -> local-1762473435997, added-data-files -> 1, added-records -> 1, added-files-size -> 1399, changed-partition-count -> 1, total-records -> 10, total-files-size -> 13934, total-data-files -> 10, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762473435997, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}                                        |\n",
      "|4630771592108515269|2025-11-06 23:57:52.181|replace  |{added-data-files -> 1, deleted-data-files -> 10, added-records -> 10, deleted-records -> 10, added-files-size -> 1595, removed-files-size -> 13934, changed-partition-count -> 1, total-records -> 10, total-files-size -> 1595, total-data-files -> 1, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1762473435997, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}|\n",
      "+-------------------+-----------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "Observe que a compacta√ß√£o criou um novo snapshot com opera√ß√£o 'replace'\n"
     ]
    }
   ],
   "source": [
    "# Verificar snapshots criados\n",
    "print(\"=== HIST√ìRICO DE SNAPSHOTS ===\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        snapshot_id,\n",
    "        committed_at,\n",
    "        operation,\n",
    "        summary\n",
    "    FROM hadoop_catalog.default.vendas.snapshots \n",
    "    ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\nObserve que a compacta√ß√£o criou um novo snapshot com opera√ß√£o 'replace'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Limpeza de Snapshots Antigos\n",
    "\n",
    "Ap√≥s a compacta√ß√£o, podemos limpar snapshots antigos para liberar espa√ßo em disco. Esta opera√ß√£o remove arquivos que n√£o s√£o mais referenciados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tOsg0upRW203"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando limpeza de snapshots antigos...\n",
      "\n",
      "Resultado da limpeza:\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "|deleted_data_files_count|deleted_position_delete_files_count|deleted_equality_delete_files_count|deleted_manifest_files_count|deleted_manifest_lists_count|deleted_statistics_files_count|\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "|                       0|                                  0|                                  0|                           0|                           0|                             0|\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "\n",
      "Limpeza conclu√≠da! Arquivos antigos foram removidos.\n"
     ]
    }
   ],
   "source": [
    "# Limpeza de snapshots antigos\n",
    "print(\"Executando limpeza de snapshots antigos...\")\n",
    "\n",
    "# Manter apenas o √∫ltimo snapshot\n",
    "cleanup_result = spark.sql(\"\"\"\n",
    "    CALL hadoop_catalog.system.expire_snapshots(\n",
    "        table => 'default.vendas',\n",
    "        retain_last => 1\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nResultado da limpeza:\")\n",
    "cleanup_result.show()\n",
    "\n",
    "print(\"Limpeza conclu√≠da! Arquivos antigos foram removidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Verifica√ß√£o Final\n",
    "\n",
    "Vamos fazer uma verifica√ß√£o final para confirmar que os dados est√£o √≠ntegros e a estrutura foi otimizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFICA√á√ÉO FINAL ===\n",
      "Dados na tabela:\n",
      "+---+----------+----------+-----+----------+\n",
      "| id|   produto|quantidade|preco|data_venda|\n",
      "+---+----------+----------+-----+----------+\n",
      "|  1| Produto 1|         2| 10.0|2024-11-01|\n",
      "|  2| Produto 2|         4| 20.0|2024-11-02|\n",
      "|  3| Produto 3|         6| 30.0|2024-11-03|\n",
      "|  4| Produto 4|         8| 40.0|2024-11-04|\n",
      "|  5| Produto 5|        10| 50.0|2024-11-05|\n",
      "|  6| Produto 6|        12| 60.0|2024-11-06|\n",
      "|  7| Produto 7|        14| 70.0|2024-11-07|\n",
      "|  8| Produto 8|        16| 80.0|2024-11-08|\n",
      "|  9| Produto 9|        18| 90.0|2024-11-09|\n",
      "| 10|Produto 10|        20|100.0|2024-11-10|\n",
      "+---+----------+----------+-----+----------+\n",
      "\n",
      "\n",
      "Informa√ß√µes finais dos arquivos:\n",
      "+-----------+-------------+--------------------+-------------+----------------+\n",
      "|total_files|total_records|avg_records_per_file|total_size_kb|avg_file_size_kb|\n",
      "+-----------+-------------+--------------------+-------------+----------------+\n",
      "|          1|           10|                10.0|         1.56|            1.56|\n",
      "+-----------+-------------+--------------------+-------------+----------------+\n",
      "\n",
      "\n",
      "Snapshots restantes ap√≥s limpeza:\n",
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|6730901837367380239|2025-11-06 23:57:30.827|append   |\n",
      "|8332996358247609723|2025-11-06 23:57:31.144|append   |\n",
      "|7004162128840245634|2025-11-06 23:57:31.391|append   |\n",
      "|7824859467824520240|2025-11-06 23:57:31.58 |append   |\n",
      "|3146811541287673173|2025-11-06 23:57:31.848|append   |\n",
      "|1557981862336165702|2025-11-06 23:57:32.035|append   |\n",
      "|1619525338135577957|2025-11-06 23:57:32.22 |append   |\n",
      "|7100358377131212126|2025-11-06 23:57:32.481|append   |\n",
      "|6116861660554806607|2025-11-06 23:57:32.657|append   |\n",
      "|9020866284305659372|2025-11-06 23:57:32.793|append   |\n",
      "|4630771592108515269|2025-11-06 23:57:52.181|replace  |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifica√ß√£o final dos dados\n",
    "print(\"=== VERIFICA√á√ÉO FINAL ===\")\n",
    "\n",
    "# Dados ainda est√£o corretos?\n",
    "print(\"Dados na tabela:\")\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas ORDER BY id\").show()\n",
    "\n",
    "# Informa√ß√µes finais dos arquivos\n",
    "print(\"\\nInforma√ß√µes finais dos arquivos:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_files,\n",
    "        SUM(record_count) as total_records,\n",
    "        AVG(record_count) as avg_records_per_file,\n",
    "        ROUND(SUM(file_size_in_bytes) / 1024.0, 2) as total_size_kb,\n",
    "        ROUND(AVG(file_size_in_bytes) / 1024.0, 2) as avg_file_size_kb\n",
    "    FROM hadoop_catalog.default.vendas.files\n",
    "\"\"\").show()\n",
    "\n",
    "# Snapshots restantes\n",
    "print(\"\\nSnapshots restantes ap√≥s limpeza:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        snapshot_id,\n",
    "        committed_at,\n",
    "        operation\n",
    "    FROM hadoop_catalog.default.vendas.snapshots \n",
    "    ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo dos Benef√≠cios da Compacta√ß√£o\n",
    "\n",
    "### Resultados Demonstrados:\n",
    "\n",
    "1. **Redu√ß√£o de Arquivos**: De 10 arquivos pequenos para 1 arquivo otimizado\n",
    "2. **Preserva√ß√£o de Dados**: Todos os registros mantidos intactos\n",
    "3. **Melhoria de Performance**: Menos overhead de I/O e metadados\n",
    "4. **Otimiza√ß√£o de Armazenamento**: Melhor compress√£o e utiliza√ß√£o de espa√ßo\n",
    "\n",
    "### Opera√ß√µes de Manuten√ß√£o Iceberg:\n",
    "\n",
    "- ‚úÖ **rewrite_data_files**: Compacta√ß√£o de arquivos pequenos\n",
    "- ‚úÖ **expire_snapshots**: Limpeza de vers√µes antigas\n",
    "- üîÑ **rewrite_manifests**: Otimiza√ß√£o de manifests\n",
    "- üîÑ **remove_orphan_files**: Remo√ß√£o de arquivos √≥rf√£os\n",
    "\n",
    "### Estrat√©gias de Compacta√ß√£o:\n",
    "\n",
    "1. **Autom√°tica**: Configurar triggers baseados em m√©tricas\n",
    "2. **Peri√≥dica**: Jobs schedulados (di√°rio, semanal)\n",
    "3. **Baseada em Eventos**: Ap√≥s grandes cargas de dados\n",
    "4. **Condicional**: Quando n√∫mero de arquivos excede threshold\n",
    "\n",
    "### Considera√ß√µes para Produ√ß√£o:\n",
    "\n",
    "- **Timing**: Executar durante janelas de baixa utiliza√ß√£o\n",
    "- **Recursos**: Compacta√ß√£o pode ser intensiva em CPU/I/O\n",
    "- **Monitoramento**: Acompanhar m√©tricas de arquivos e performance\n",
    "- **Backup**: Manter snapshots cr√≠ticos antes da limpeza\n",
    "- **Testes**: Validar integridade ap√≥s compacta√ß√£o\n",
    "\n",
    "### M√©tricas de Monitoramento:\n",
    "\n",
    "- N√∫mero de arquivos por tabela\n",
    "- Tamanho m√©dio dos arquivos\n",
    "- Tempo de execu√ß√£o de queries\n",
    "- Utiliza√ß√£o de armazenamento\n",
    "- Frequ√™ncia de compacta√ß√£o necess√°ria"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNR5DDJ98pjkAvr9uwdOAth",
   "provenance": [
    {
     "file_id": "1NynsLfy1R-f8yQsXcpO1G7gPWRzUXxRI",
     "timestamp": 1731248613539
    },
    {
     "file_id": "1RbjXXITM0ttvuoaFrNaUZwtefAupJ1rc",
     "timestamp": 1731245177251
    },
    {
     "file_id": "178ZW6B8jLzISq4ceO5QdkIi2ZG-ybMAp",
     "timestamp": 1730934676588
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
