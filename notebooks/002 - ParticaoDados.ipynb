{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dNQHqJs0mPv"
   },
   "source": [
    "# Particionamento de Dados com Apache Iceberg\n",
    "\n",
    "O particionamento é uma técnica fundamental para otimizar consultas em grandes volumes de dados. O Apache Iceberg oferece recursos avançados como **Particionamento Oculto** e **Evolução de Particionamento**.\n",
    "\n",
    "## Conceitos Principais:\n",
    "\n",
    "### 1. Particionamento Oculto (Hidden Partitioning)\n",
    "- O Iceberg abstrai a complexidade do particionamento\n",
    "- Usuário define apenas a coluna e função de transformação (ex: `year(data_venda)`)\n",
    "- Não precisa incluir colunas derivadas nas consultas\n",
    "- Simplifica queries e evita erros\n",
    "\n",
    "### 2. Evolução de Particionamento (Partition Evolution)\n",
    "- Permite alterar estratégia de particionamento sem reescrever dados\n",
    "- Novos dados usam novo esquema, dados antigos mantêm esquema original\n",
    "- Adapta-se ao crescimento e mudanças nos padrões de consulta\n",
    "\n",
    "## Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3tZdZJzd7N8"
   },
   "outputs": [],
   "source": [
    "# Para o Spark se estiver rodando\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Configuração do Spark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.3.0-bin-hadoop3\"\n",
    "\n",
    "import findspark\n",
    "findspark.init('/opt/spark-3.3.0-bin-hadoop3')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Sessão Spark com configuração explícita\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergPartitioning\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", \"file:///home/tavares/warehouse\") \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Warehouse configurado para: {spark.conf.get('spark.sql.catalog.hadoop_catalog.warehouse')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmje1NJ_uYAi"
   },
   "source": [
    "## 1. Particionamento por Ano\n",
    "\n",
    "Vamos criar uma tabela particionada por ano usando a função `year(data_venda)`. Isso permite que consultas filtradas por ano leiam apenas os arquivos relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar tabela particionada por ano da data de venda\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hadoop_catalog.default.vendas_partitioned (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (year(data_venda))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserindo Dados de Diferentes Anos\n",
    "\n",
    "Vamos inserir dados de 2022, 2023 e 2024 para demonstrar como o Iceberg organiza automaticamente os dados em partições por ano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserir dados de diferentes anos\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas_partitioned VALUES\n",
    "    (1, 'Produto A', 10, 15.5, DATE('2022-01-15')),\n",
    "    (2, 'Produto B', 5, 22.0, DATE('2022-06-20')),\n",
    "    (3, 'Produto C', 8, 30.0, DATE('2023-03-10')),\n",
    "    (4, 'Produto D', 12, 25.0, DATE('2023-08-25')),\n",
    "    (5, 'Produto E', 7, 18.5, DATE('2024-02-14'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando os Dados\n",
    "\n",
    "Observe que consultamos diretamente a coluna `data_venda` sem precisar especificar a partição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar dados\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_partitioned ORDER BY data_venda\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando as Partições\n",
    "\n",
    "O Iceberg cria automaticamente partições baseadas no ano. Cada partição contém estatísticas úteis para otimização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver informações sobre partições\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_partitioned.partitions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta Otimizada por Partição\n",
    "\n",
    "Esta consulta lerá apenas os arquivos da partição de 2023, demonstrando o **partition pruning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta que aproveita particionamento (apenas dados de 2023)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM hadoop_catalog.default.vendas_partitioned \n",
    "    WHERE year(data_venda) = 2023\n",
    "    ORDER BY data_venda\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando Arquivos por Partição\n",
    "\n",
    "Podemos ver como os arquivos estão organizados fisicamente por partição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver arquivos da tabela particionada\n",
    "spark.sql(\"SELECT file_path, partition FROM hadoop_catalog.default.vendas_partitioned.files\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Particionamento por Dias\n",
    "\n",
    "Para dados com alta granularidade temporal, podemos usar `days(data_venda)` que particiona por dia desde a época Unix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar tabela com particionamento por dias (evita conflito year/month)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hadoop_catalog.default.vendas_daily (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(data_venda))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserindo Dados na Tabela Diária\n",
    "\n",
    "Cada data única criará uma partição separada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserir dados na tabela com particionamento diário\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas_daily VALUES\n",
    "    (1, 'Produto A', 10, 15.5, DATE('2023-01-15')),\n",
    "    (2, 'Produto B', 5, 22.0, DATE('2023-01-20')),\n",
    "    (3, 'Produto C', 8, 30.0, DATE('2023-02-10')),\n",
    "    (4, 'Produto D', 12, 25.0, DATE('2023-02-25')),\n",
    "    (5, 'Produto E', 7, 18.5, DATE('2023-03-14'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando Partições Diárias\n",
    "\n",
    "Observe que cada data única cria uma partição com número de dias desde 1970-01-01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver partições da tabela diária\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_daily.partitions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Particionamento por Bucket\n",
    "\n",
    "O particionamento por bucket distribui dados uniformemente usando uma função hash. Útil para balanceamento de carga e joins eficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar tabela com particionamento por bucket (alternativa para múltiplas dimensões)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hadoop_catalog.default.vendas_bucket (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (bucket(4, id))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserindo Dados com Bucket\n",
    "\n",
    "Os dados serão distribuídos em 4 buckets baseados no hash do campo `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserir dados na tabela com bucket\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas_bucket VALUES\n",
    "    (1, 'Produto A', 10, 15.5, DATE('2023-01-15')),\n",
    "    (2, 'Produto B', 5, 22.0, DATE('2023-01-20')),\n",
    "    (3, 'Produto C', 8, 30.0, DATE('2023-02-10')),\n",
    "    (4, 'Produto D', 12, 25.0, DATE('2023-02-25')),\n",
    "    (5, 'Produto E', 7, 18.5, DATE('2023-03-14'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando Partições por Bucket\n",
    "\n",
    "Cada bucket (0-3) contém dados distribuídos uniformemente pelo hash do ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver partições por bucket\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_bucket.partitions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Particionamento por Categoria\n",
    "\n",
    "Particionamento por valores categóricos é útil quando temos consultas frequentes filtradas por categoria específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de particionamento por categoria de produto\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hadoop_catalog.default.vendas_categoria (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        categoria STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (categoria)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserindo Dados com Categorias\n",
    "\n",
    "Cada categoria criará uma partição separada, otimizando consultas por categoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserir dados com categorias\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas_categoria VALUES\n",
    "    (1, 'Notebook', 'Eletrônicos', 2, 2500.0, DATE('2023-01-15')),\n",
    "    (2, 'Mesa', 'Móveis', 1, 800.0, DATE('2023-01-20')),\n",
    "    (3, 'Smartphone', 'Eletrônicos', 3, 1200.0, DATE('2023-02-10')),\n",
    "    (4, 'Cadeira', 'Móveis', 4, 300.0, DATE('2023-02-25')),\n",
    "    (5, 'Tablet', 'Eletrônicos', 1, 600.0, DATE('2023-03-14'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta Otimizada por Categoria\n",
    "\n",
    "Esta consulta lerá apenas os arquivos da partição 'Eletrônicos', ignorando dados de outras categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultar apenas eletrônicos\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM hadoop_catalog.default.vendas_categoria \n",
    "    WHERE categoria = 'Eletrônicos'\n",
    "    ORDER BY data_venda\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando Partições por Categoria\n",
    "\n",
    "Podemos ver as partições criadas para cada categoria única."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver partições por categoria\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_categoria.partitions\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}