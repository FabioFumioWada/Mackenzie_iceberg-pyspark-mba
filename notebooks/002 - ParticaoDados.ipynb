{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dNQHqJs0mPv"
   },
   "source": [
    "# Particionamento de Dados com Apache Iceberg\n",
    "\n",
    "O particionamento é uma técnica fundamental para otimizar consultas em grandes volumes de dados. O Apache Iceberg oferece recursos avançados como **Particionamento Oculto** e **Evolução de Particionamento**.\n",
    "\n",
    "## Conceitos Principais:\n",
    "\n",
    "### 1. Particionamento Oculto (Hidden Partitioning)\n",
    "- O Iceberg abstrai a complexidade do particionamento\n",
    "- Usuário define apenas a coluna e função de transformação (ex: `year(data_venda)`)\n",
    "- Não precisa incluir colunas derivadas nas consultas\n",
    "- Simplifica queries e evita erros\n",
    "\n",
    "### 2. Evolução de Particionamento (Partition Evolution)\n",
    "- Permite alterar estratégia de particionamento sem reescrever dados\n",
    "- Novos dados usam novo esquema, dados antigos mantêm esquema original\n",
    "- Adapta-se ao crescimento e mudanças nos padrões de consulta\n",
    "\n",
    "## Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-3tZdZJzd7N8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse configurado para: file:///home/tavares/warehouse\n"
     ]
    }
   ],
   "source": [
    "# Para o Spark se estiver rodando\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Configuração do Spark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.3.0-bin-hadoop3\"\n",
    "\n",
    "import findspark\n",
    "findspark.init('/opt/spark-3.3.0-bin-hadoop3')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Sessão Spark com configuração explícita\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergPartitioning\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", \"file:///home/tavares/warehouse\") \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Warehouse configurado para: {spark.conf.get('spark.sql.catalog.hadoop_catalog.warehouse')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmje1NJ_uYAi"
   },
   "source": [
    "## 1. Particionamento por Ano\n",
    "\n",
    "Vamos criar uma tabela particionada por ano usando a função `year(data_venda)`. Isso permite que consultas filtradas por ano leiam apenas os arquivos relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar tabela particionada por ano da data de venda\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hadoop_catalog.default.vendas_partitioned (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (year(data_venda))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserindo Dados de Diferentes Anos\n",
    "\n",
    "Vamos inserir dados de 2022, 2023 e 2024 para demonstrar como o Iceberg organiza automaticamente os dados em partições por ano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserir dados de diferentes anos\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas_partitioned VALUES\n",
    "    (1, 'Produto A', 10, 15.5, DATE('2022-01-15')),\n",
    "    (2, 'Produto B', 5, 22.0, DATE('2022-06-20')),\n",
    "    (3, 'Produto C', 8, 30.0, DATE('2023-03-10')),\n",
    "    (4, 'Produto D', 12, 25.0, DATE('2023-08-25')),\n",
    "    (5, 'Produto E', 7, 18.5, DATE('2024-02-14'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando os Dados\n",
    "\n",
    "Observe que consultamos diretamente a coluna `data_venda` sem precisar especificar a partição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2022-01-15|\n",
      "|  2|Produto B|         5| 22.0|2022-06-20|\n",
      "|  3|Produto C|         8| 30.0|2023-03-10|\n",
      "|  4|Produto D|        12| 25.0|2023-08-25|\n",
      "|  5|Produto E|         7| 18.5|2024-02-14|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizar dados\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_partitioned ORDER BY data_venda\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando as Partições\n",
    "\n",
    "O Iceberg cria automaticamente partições baseadas no ano. Cada partição contém estatísticas úteis para otimização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|     {54}|      0|           1|         1|                         1393|                           0|                         0|                           0|                         0|2025-11-03 01:03:...|     1286999254922226430|\n",
      "|     {52}|      0|           2|         1|                         1397|                           0|                         0|                           0|                         0|2025-11-03 01:03:...|     1286999254922226430|\n",
      "|     {53}|      0|           2|         1|                         1397|                           0|                         0|                           0|                         0|2025-11-03 01:03:...|     1286999254922226430|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver informações sobre partições\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_partitioned.partitions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta Otimizada por Partição\n",
    "\n",
    "Esta consulta lerá apenas os arquivos da partição de 2023, demonstrando o **partition pruning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  3|Produto C|         8| 30.0|2023-03-10|\n",
      "|  4|Produto D|        12| 25.0|2023-08-25|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta que aproveita particionamento (apenas dados de 2023)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM hadoop_catalog.default.vendas_partitioned \n",
    "    WHERE year(data_venda) = 2023\n",
    "    ORDER BY data_venda\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando Arquivos por Partição\n",
    "\n",
    "Podemos ver como os arquivos estão organizados fisicamente por partição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|file_path                                                                                                                                     |partition|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "|file:/home/tavares/warehouse/default/vendas_partitioned/data/data_venda_year=2023/00005-5-9059b484-a4c6-4aa7-ba7d-b4ca57677dde-0-00001.parquet|{53}     |\n",
      "|file:/home/tavares/warehouse/default/vendas_partitioned/data/data_venda_year=2022/00044-6-9059b484-a4c6-4aa7-ba7d-b4ca57677dde-0-00001.parquet|{52}     |\n",
      "|file:/home/tavares/warehouse/default/vendas_partitioned/data/data_venda_year=2024/00061-7-9059b484-a4c6-4aa7-ba7d-b4ca57677dde-0-00001.parquet|{54}     |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver arquivos da tabela particionada\n",
    "spark.sql(\"SELECT file_path, partition FROM hadoop_catalog.default.vendas_partitioned.files\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Particionamento por Dias\n",
    "\n",
    "Para dados com alta granularidade temporal, podemos usar `days(data_venda)` que particiona por dia desde a época Unix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar tabela com particionamento por dias (evita conflito year/month)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hadoop_catalog.default.vendas_daily (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(data_venda))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserindo Dados na Tabela Diária\n",
    "\n",
    "Cada data única criará uma partição separada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserir dados na tabela com particionamento diário\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas_daily VALUES\n",
    "    (1, 'Produto A', 10, 15.5, DATE('2023-01-15')),\n",
    "    (2, 'Produto B', 5, 22.0, DATE('2023-01-20')),\n",
    "    (3, 'Produto C', 8, 30.0, DATE('2023-02-10')),\n",
    "    (4, 'Produto D', 12, 25.0, DATE('2023-02-25')),\n",
    "    (5, 'Produto E', 7, 18.5, DATE('2023-03-14'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando Partições Diárias\n",
    "\n",
    "Observe que cada data única cria uma partição com número de dias desde 1970-01-01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|   partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|{2023-02-10}|      0|           1|         1|                         1391|                           0|                         0|                           0|                         0|2025-11-03 01:04:...|     2485143199939475997|\n",
      "|{2023-03-14}|      0|           1|         1|                         1393|                           0|                         0|                           0|                         0|2025-11-03 01:04:...|     2485143199939475997|\n",
      "|{2023-01-15}|      0|           1|         1|                         1393|                           0|                         0|                           0|                         0|2025-11-03 01:04:...|     2485143199939475997|\n",
      "|{2023-01-20}|      0|           1|         1|                         1392|                           0|                         0|                           0|                         0|2025-11-03 01:04:...|     2485143199939475997|\n",
      "|{2023-02-25}|      0|           1|         1|                         1393|                           0|                         0|                           0|                         0|2025-11-03 01:04:...|     2485143199939475997|\n",
      "+------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver partições da tabela diária\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_daily.partitions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Particionamento por Bucket\n",
    "\n",
    "O particionamento por bucket distribui dados uniformemente usando uma função hash. Útil para balanceamento de carga e joins eficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar tabela com particionamento por bucket (alternativa para múltiplas dimensões)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hadoop_catalog.default.vendas_bucket (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (bucket(4, id))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserindo Dados com Bucket\n",
    "\n",
    "Os dados serão distribuídos em 4 buckets baseados no hash do campo `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserir dados na tabela com bucket\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas_bucket VALUES\n",
    "    (1, 'Produto A', 10, 15.5, DATE('2023-01-15')),\n",
    "    (2, 'Produto B', 5, 22.0, DATE('2023-01-20')),\n",
    "    (3, 'Produto C', 8, 30.0, DATE('2023-02-10')),\n",
    "    (4, 'Produto D', 12, 25.0, DATE('2023-02-25')),\n",
    "    (5, 'Produto E', 7, 18.5, DATE('2023-03-14'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando Partições por Bucket\n",
    "\n",
    "Cada bucket (0-3) contém dados distribuídos uniformemente pelo hash do ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|      {0}|      0|           2|         1|                         1398|                           0|                         0|                           0|                         0|2025-11-03 01:04:...|     1726455918655621727|\n",
      "|      {2}|      0|           1|         1|                         1393|                           0|                         0|                           0|                         0|2025-11-03 01:04:...|     1726455918655621727|\n",
      "|      {3}|      0|           2|         1|                         1398|                           0|                         0|                           0|                         0|2025-11-03 01:04:...|     1726455918655621727|\n",
      "+---------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver partições por bucket\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_bucket.partitions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Particionamento por Categoria\n",
    "\n",
    "Particionamento por valores categóricos é útil quando temos consultas frequentes filtradas por categoria específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo de particionamento por categoria de produto\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hadoop_catalog.default.vendas_categoria (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        categoria STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (categoria)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserindo Dados com Categorias\n",
    "\n",
    "Cada categoria criará uma partição separada, otimizando consultas por categoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserir dados com categorias\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas_categoria VALUES\n",
    "    (1, 'Notebook', 'Eletrônicos', 2, 2500.0, DATE('2023-01-15')),\n",
    "    (2, 'Mesa', 'Móveis', 1, 800.0, DATE('2023-01-20')),\n",
    "    (3, 'Smartphone', 'Eletrônicos', 3, 1200.0, DATE('2023-02-10')),\n",
    "    (4, 'Cadeira', 'Móveis', 4, 300.0, DATE('2023-02-25')),\n",
    "    (5, 'Tablet', 'Eletrônicos', 1, 600.0, DATE('2023-03-14'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta Otimizada por Categoria\n",
    "\n",
    "Esta consulta lerá apenas os arquivos da partição 'Eletrônicos', ignorando dados de outras categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+----------+------+----------+\n",
      "| id|   produto|  categoria|quantidade| preco|data_venda|\n",
      "+---+----------+-----------+----------+------+----------+\n",
      "|  1|  Notebook|Eletrônicos|         2|2500.0|2023-01-15|\n",
      "|  3|Smartphone|Eletrônicos|         3|1200.0|2023-02-10|\n",
      "|  5|    Tablet|Eletrônicos|         1| 600.0|2023-03-14|\n",
      "+---+----------+-----------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consultar apenas eletrônicos\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM hadoop_catalog.default.vendas_categoria \n",
    "    WHERE categoria = 'Eletrônicos'\n",
    "    ORDER BY data_venda\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando Partições por Categoria\n",
    "\n",
    "Podemos ver as partições criadas para cada categoria única."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|    partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+-------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|     {Móveis}|      0|           2|         1|                         1686|                           0|                         0|                           0|                         0|2025-11-03 01:04:...|     9072823416610609881|\n",
      "|{Eletrônicos}|      0|           3|         1|                         1765|                           0|                         0|                           0|                         0|2025-11-03 01:04:...|     9072823416610609881|\n",
      "+-------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver partições por categoria\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_categoria.partitions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
