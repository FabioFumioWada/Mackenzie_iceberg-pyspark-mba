{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dNQHqJs0mPv"
   },
   "source": [
    "# Evolução de Schema com Apache Iceberg\n",
    "\n",
    "Este notebook demonstra uma das funcionalidades mais poderosas do Apache Iceberg: a capacidade de evoluir o schema de tabelas sem interrupção de serviço ou reescrita de dados.\n",
    "\n",
    "## Contexto e Objetivos:\n",
    "\n",
    "### Desafios Tradicionais com Schema:\n",
    "- **Formatos Tradicionais**: Parquet, ORC requerem reescrita completa para mudanças de schema\n",
    "- **Downtime**: Necessidade de parar aplicações durante alterações\n",
    "- **Complexidade**: Processos manuais e propensos a erro\n",
    "- **Custo**: Reprocessamento de grandes volumes de dados\n",
    "\n",
    "### Benefícios da Schema Evolution no Iceberg:\n",
    "- **Zero Downtime**: Alterações sem interrupção de serviço\n",
    "- **Compatibilidade**: Dados antigos permanecem acessíveis\n",
    "- **Flexibilidade**: Adição, remoção e renomeação de colunas\n",
    "- **Versionamento**: Histórico completo de mudanças de schema\n",
    "- **Performance**: Sem necessidade de reescrita de dados\n",
    "\n",
    "### Tipos de Evolução Suportados:\n",
    "1. **ADD COLUMN**: Adicionar novas colunas\n",
    "2. **DROP COLUMN**: Remover colunas existentes\n",
    "3. **RENAME COLUMN**: Renomear colunas\n",
    "4. **ALTER COLUMN TYPE**: Mudanças compatíveis de tipo\n",
    "5. **REORDER COLUMNS**: Reorganizar ordem das colunas\n",
    "\n",
    "## Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-3tZdZJzd7N8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse configurado para: file:///home/tavares/warehouse\n"
     ]
    }
   ],
   "source": [
    "# Para o Spark se estiver rodando\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Configuração do Spark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.3.0-bin-hadoop3\"\n",
    "\n",
    "import findspark\n",
    "findspark.init('/opt/spark-3.3.0-bin-hadoop3')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergSchemaEvolution\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", \"file:///home/tavares/warehouse\") \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Warehouse configurado para: {spark.conf.get('spark.sql.catalog.hadoop_catalog.warehouse')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZrbavgI1b9Z"
   },
   "source": [
    "## 1. Criação da Tabela Inicial\n",
    "\n",
    "Vamos começar criando uma tabela com um schema básico que será evoluído ao longo do exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tmPQCmoeeHsj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exclui a tabela se existir\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.vendas\")\n",
    "\n",
    "# Cria a tabela Vendas no catálogo, usando Iceberg\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE hadoop_catalog.default.vendas (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inserção de Dados Iniciais\n",
    "\n",
    "Vamos inserir alguns dados na tabela com o schema original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G0773bqNegO8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Incluindo dados na tabela vendas usando SQL puro\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas VALUES\n",
    "    (1, 'Produto A', 10, 15.5, DATE('2024-11-01')),\n",
    "    (2, 'Produto B', 5, 22.0, DATE('2024-11-02')),\n",
    "    (3, 'Produto C', 8, 30.0, DATE('2024-11-03'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verificação dos Dados Iniciais\n",
    "\n",
    "Vamos verificar os dados inseridos e o schema atual da tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4ZavjMV5eSp_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dados Iniciais ===\n",
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2024-11-01|\n",
      "|  2|Produto B|         5| 22.0|2024-11-02|\n",
      "|  3|Produto C|         8| 30.0|2024-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n",
      "\n",
      "=== Schema Inicial ===\n",
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|             id|      int|       |\n",
      "|        produto|   string|       |\n",
      "|     quantidade|      int|       |\n",
      "|          preco|   double|       |\n",
      "|     data_venda|     date|       |\n",
      "|               |         |       |\n",
      "| # Partitioning|         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificamos os dados\n",
    "print(\"=== Dados Iniciais ===\")\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas\").show()\n",
    "\n",
    "# Verificamos o schema atual\n",
    "print(\"\\n=== Schema Inicial ===\")\n",
    "spark.sql(\"DESCRIBE hadoop_catalog.default.vendas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlO9-vecQwh2"
   },
   "source": [
    "## 4. Evolução do Schema - Adicionando Coluna\n",
    "\n",
    "Agora vamos demonstrar a primeira evolução: adicionar uma nova coluna `desconto` à tabela existente. Esta operação é instantânea e não requer reescrita dos dados existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MiepqlhJciVN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coluna 'desconto' adicionada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Adicionando nova coluna desconto\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hadoop_catalog.default.vendas\n",
    "    ADD COLUMN desconto DOUBLE\n",
    "\"\"\")\n",
    "\n",
    "print(\"Coluna 'desconto' adicionada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verificação do Schema Evoluído\n",
    "\n",
    "Vamos verificar como o schema foi alterado e como os dados existentes são tratados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5mciZpSiRXUU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Schema Após Adição de Coluna ===\n",
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|             id|      int|       |\n",
      "|        produto|   string|       |\n",
      "|     quantidade|      int|       |\n",
      "|          preco|   double|       |\n",
      "|     data_venda|     date|       |\n",
      "|       desconto|   double|       |\n",
      "|               |         |       |\n",
      "| # Partitioning|         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n",
      "\n",
      "=== Dados Após Evolução (note os valores NULL) ===\n",
      "+---+---------+----------+-----+----------+--------+\n",
      "| id|  produto|quantidade|preco|data_venda|desconto|\n",
      "+---+---------+----------+-----+----------+--------+\n",
      "|  1|Produto A|        10| 15.5|2024-11-01|    null|\n",
      "|  2|Produto B|         5| 22.0|2024-11-02|    null|\n",
      "|  3|Produto C|         8| 30.0|2024-11-03|    null|\n",
      "+---+---------+----------+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema Atualizado\n",
    "print(\"=== Schema Após Adição de Coluna ===\")\n",
    "spark.sql(\"DESCRIBE hadoop_catalog.default.vendas\").show()\n",
    "\n",
    "# Verificar dados existentes (nova coluna será NULL)\n",
    "print(\"\\n=== Dados Após Evolução (note os valores NULL) ===\")\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inserção de Dados com Nova Coluna\n",
    "\n",
    "Agora vamos inserir novos dados que incluem valores para a coluna `desconto` recém-adicionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qHTLkWxuRXZ0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novos dados inseridos com valores de desconto!\n"
     ]
    }
   ],
   "source": [
    "# Inserindo dados com a nova coluna\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas VALUES\n",
    "    (4, 'Produto D', 12, 25.0, DATE('2024-11-04'), 2.5),\n",
    "    (5, 'Produto E', 7, 18.0, DATE('2024-11-05'), 1.0)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Novos dados inseridos com valores de desconto!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verificação Final dos Dados\n",
    "\n",
    "Vamos ver como a tabela agora contém dados com e sem a nova coluna, demonstrando a compatibilidade retroativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NIgjHxU1Rb99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dados Finais (Mistura de Registros Antigos e Novos) ===\n",
      "+---+---------+----------+-----+----------+--------+\n",
      "| id|  produto|quantidade|preco|data_venda|desconto|\n",
      "+---+---------+----------+-----+----------+--------+\n",
      "|  1|Produto A|        10| 15.5|2024-11-01|    null|\n",
      "|  2|Produto B|         5| 22.0|2024-11-02|    null|\n",
      "|  3|Produto C|         8| 30.0|2024-11-03|    null|\n",
      "|  4|Produto D|        12| 25.0|2024-11-04|     2.5|\n",
      "|  5|Produto E|         7| 18.0|2024-11-05|     1.0|\n",
      "+---+---------+----------+-----+----------+--------+\n",
      "\n",
      "\n",
      "=== Estatísticas ===\n",
      "+---------------+\n",
      "|total_registros|\n",
      "+---------------+\n",
      "|              5|\n",
      "+---------------+\n",
      "\n",
      "+------------+\n",
      "|com_desconto|\n",
      "+------------+\n",
      "|           2|\n",
      "+------------+\n",
      "\n",
      "+------------+\n",
      "|sem_desconto|\n",
      "+------------+\n",
      "|           3|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consultando todos os dados\n",
    "print(\"=== Dados Finais (Mistura de Registros Antigos e Novos) ===\")\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas ORDER BY id\").show()\n",
    "\n",
    "# Estatísticas da tabela\n",
    "print(\"\\n=== Estatísticas ===\")\n",
    "spark.sql(\"SELECT COUNT(*) as total_registros FROM hadoop_catalog.default.vendas\").show()\n",
    "spark.sql(\"SELECT COUNT(*) as com_desconto FROM hadoop_catalog.default.vendas WHERE desconto IS NOT NULL\").show()\n",
    "spark.sql(\"SELECT COUNT(*) as sem_desconto FROM hadoop_catalog.default.vendas WHERE desconto IS NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mais Evoluções de Schema\n",
    "\n",
    "Vamos demonstrar outras operações de evolução de schema disponíveis no Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evoluções adicionais aplicadas!\n",
      "\n",
      "=== Schema Final ===\n",
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|             id|      int|       |\n",
      "|   nome_produto|   string|       |\n",
      "|     quantidade|      int|       |\n",
      "|          preco|   double|       |\n",
      "|     data_venda|     date|       |\n",
      "|       desconto|   double|       |\n",
      "|      categoria|   string|       |\n",
      "|               |         |       |\n",
      "| # Partitioning|         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adicionando mais uma coluna\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hadoop_catalog.default.vendas\n",
    "    ADD COLUMN categoria STRING\n",
    "\"\"\")\n",
    "\n",
    "# Renomeando uma coluna\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE hadoop_catalog.default.vendas\n",
    "    RENAME COLUMN produto TO nome_produto\n",
    "\"\"\")\n",
    "\n",
    "print(\"Evoluções adicionais aplicadas!\")\n",
    "\n",
    "# Verificar schema final\n",
    "print(\"\\n=== Schema Final ===\")\n",
    "spark.sql(\"DESCRIBE hadoop_catalog.default.vendas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inserção com Schema Completo\n",
    "\n",
    "Vamos inserir dados usando o schema completamente evoluído."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados inseridos com schema completo!\n",
      "\n",
      "=== Dados Finais com Schema Evoluído ===\n",
      "+---+------------+----------+-----+----------+--------+-----------+\n",
      "| id|nome_produto|quantidade|preco|data_venda|desconto|  categoria|\n",
      "+---+------------+----------+-----+----------+--------+-----------+\n",
      "|  1|   Produto A|        10| 15.5|2024-11-01|    null|       null|\n",
      "|  2|   Produto B|         5| 22.0|2024-11-02|    null|       null|\n",
      "|  3|   Produto C|         8| 30.0|2024-11-03|    null|       null|\n",
      "|  4|   Produto D|        12| 25.0|2024-11-04|     2.5|       null|\n",
      "|  5|   Produto E|         7| 18.0|2024-11-05|     1.0|       null|\n",
      "|  6|   Produto F|        15| 35.0|2024-11-06|     3.0|Eletrônicos|\n",
      "|  7|   Produto G|        20| 45.0|2024-11-07|     5.0|       Casa|\n",
      "+---+------------+----------+-----+----------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inserir dados com todas as colunas\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas VALUES\n",
    "    (6, 'Produto F', 15, 35.0, DATE('2024-11-06'), 3.0, 'Eletrônicos'),\n",
    "    (7, 'Produto G', 20, 45.0, DATE('2024-11-07'), 5.0, 'Casa')\n",
    "\"\"\")\n",
    "\n",
    "print(\"Dados inseridos com schema completo!\")\n",
    "\n",
    "# Visualizar resultado final\n",
    "print(\"\\n=== Dados Finais com Schema Evoluído ===\")\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas ORDER BY id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Histórico de Snapshots\n",
    "\n",
    "Vamos verificar como cada evolução de schema criou novos snapshots, permitindo auditoria completa das mudanças."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Histórico de Snapshots ===\n",
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|2785368502997395094|2025-11-02 23:13:40.555|append   |\n",
      "|4794735984909422061|2025-11-02 23:13:49.538|append   |\n",
      "|2939522097528125544|2025-11-02 23:14:17.6  |append   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n",
      "\n",
      "=== Histórico da Tabela ===\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2025-11-02 23:13:40.555|2785368502997395094|null               |true               |\n",
      "|2025-11-02 23:13:49.538|4794735984909422061|2785368502997395094|true               |\n",
      "|2025-11-02 23:14:17.6  |2939522097528125544|4794735984909422061|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar snapshots criados durante as evoluções\n",
    "print(\"=== Histórico de Snapshots ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT snapshot_id, committed_at, operation\n",
    "    FROM hadoop_catalog.default.vendas.snapshots \n",
    "    ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Verificar histórico da tabela\n",
    "print(\"\\n=== Histórico da Tabela ===\")\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Time Travel com Schemas Diferentes\n",
    "\n",
    "Demonstração de como consultar dados usando schemas de diferentes pontos no tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consultando primeiro snapshot: 2785368502997395094\n",
      "=== Dados com Schema Original ===\n",
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2024-11-01|\n",
      "|  2|Produto B|         5| 22.0|2024-11-02|\n",
      "|  3|Produto C|         8| 30.0|2024-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n",
      "\n",
      "=== Dados com Schema Atual ===\n",
      "+---+------------+----------+-----+----------+--------+-----------+\n",
      "| id|nome_produto|quantidade|preco|data_venda|desconto|  categoria|\n",
      "+---+------------+----------+-----+----------+--------+-----------+\n",
      "|  1|   Produto A|        10| 15.5|2024-11-01|    null|       null|\n",
      "|  2|   Produto B|         5| 22.0|2024-11-02|    null|       null|\n",
      "|  3|   Produto C|         8| 30.0|2024-11-03|    null|       null|\n",
      "|  4|   Produto D|        12| 25.0|2024-11-04|     2.5|       null|\n",
      "|  5|   Produto E|         7| 18.0|2024-11-05|     1.0|       null|\n",
      "|  6|   Produto F|        15| 35.0|2024-11-06|     3.0|Eletrônicos|\n",
      "|  7|   Produto G|        20| 45.0|2024-11-07|     5.0|       Casa|\n",
      "+---+------------+----------+-----+----------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consultar dados do primeiro snapshot (schema original)\n",
    "snapshots = spark.sql(\"SELECT snapshot_id FROM hadoop_catalog.default.vendas.snapshots ORDER BY committed_at\")\n",
    "first_snapshot = snapshots.collect()[0][0]\n",
    "\n",
    "print(f\"Consultando primeiro snapshot: {first_snapshot}\")\n",
    "print(\"=== Dados com Schema Original ===\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM hadoop_catalog.default.vendas \n",
    "    VERSION AS OF {first_snapshot}\n",
    "    ORDER BY id\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n=== Dados com Schema Atual ===\")\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas ORDER BY id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo dos Benefícios da Schema Evolution\n",
    "\n",
    "### Vantagens Demonstradas:\n",
    "\n",
    "1. **Zero Downtime**: Todas as alterações foram aplicadas sem interrupção\n",
    "2. **Compatibilidade Retroativa**: Dados antigos permanecem acessíveis\n",
    "3. **Flexibilidade**: Múltiplos tipos de evolução suportados\n",
    "4. **Versionamento**: Histórico completo de mudanças\n",
    "5. **Time Travel**: Acesso a dados com schemas históricos\n",
    "\n",
    "### Operações Suportadas:\n",
    "\n",
    "- ✅ **ADD COLUMN**: Adicionar novas colunas\n",
    "- ✅ **RENAME COLUMN**: Renomear colunas existentes\n",
    "- ✅ **DROP COLUMN**: Remover colunas (não demonstrado)\n",
    "- ✅ **ALTER COLUMN TYPE**: Mudanças compatíveis de tipo\n",
    "- ✅ **REORDER COLUMNS**: Reorganizar ordem das colunas\n",
    "\n",
    "### Casos de Uso Empresariais:\n",
    "\n",
    "- **Evolução de Aplicações**: Adicionar campos conforme requisitos mudam\n",
    "- **Integração de Dados**: Incorporar novas fontes de dados\n",
    "- **Compliance**: Adicionar campos para regulamentações\n",
    "- **Analytics**: Enriquecer dados para análises avançadas\n",
    "- **Migration**: Migração gradual entre schemas\n",
    "\n",
    "### Considerações Importantes:\n",
    "\n",
    "- **Compatibilidade**: Nem todas as mudanças de tipo são suportadas\n",
    "- **Performance**: Schemas muito largos podem impactar performance\n",
    "- **Planejamento**: Evoluções devem ser planejadas considerando aplicações downstream\n",
    "- **Testes**: Sempre testar evoluções em ambiente de desenvolvimento primeiro"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNIecPpSISXGaghsJiVqYYB",
   "provenance": [
    {
     "file_id": "1RbjXXITM0ttvuoaFrNaUZwtefAupJ1rc",
     "timestamp": 1731245177251
    },
    {
     "file_id": "178ZW6B8jLzISq4ceO5QdkIi2ZG-ybMAp",
     "timestamp": 1730934676588
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
