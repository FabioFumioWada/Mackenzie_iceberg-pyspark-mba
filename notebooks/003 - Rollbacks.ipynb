{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dNQHqJs0mPv"
   },
   "source": [
    "# Rollbacks e Versionamento com Apache Iceberg\n",
    "\n",
    "O Apache Iceberg oferece capacidades avançadas de versionamento e rollback que permitem:\n",
    "\n",
    "## Conceitos Principais:\n",
    "\n",
    "### 1. Snapshots\n",
    "- Cada operação (INSERT, UPDATE, DELETE) cria um novo snapshot\n",
    "- Snapshots são imutáveis e contêm metadados completos da tabela\n",
    "- Permitem consultas pontuais no tempo (time travel)\n",
    "\n",
    "### 2. Rollbacks\n",
    "- Capacidade de reverter tabela para um snapshot anterior\n",
    "- Operação rápida que apenas atualiza metadados\n",
    "- Não requer reescrita de dados\n",
    "\n",
    "### 3. Versionamento\n",
    "- Histórico completo de mudanças na tabela\n",
    "- Rastreabilidade de operações\n",
    "- Recuperação de dados em caso de erro\n",
    "\n",
    "## Setup do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-3tZdZJzd7N8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse configurado para: file:///home/tavares/warehouse\n"
     ]
    }
   ],
   "source": [
    "# Para o Spark se estiver rodando\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Configuração do Spark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-3.3.0-bin-hadoop3\"\n",
    "\n",
    "import findspark\n",
    "findspark.init('/opt/spark-3.3.0-bin-hadoop3')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Sessão Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergRollbacks\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", \"file:///home/tavares/warehouse\") \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Warehouse configurado para: {spark.conf.get('spark.sql.catalog.hadoop_catalog.warehouse')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x89UY3vxGAd"
   },
   "source": [
    "## 1. Criação da Tabela de Exemplo\n",
    "\n",
    "Vamos criar uma tabela para demonstrar os conceitos de versionamento e rollback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5E3eNFZsxKbv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criação da Tabela\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.vendas_versioned\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE hadoop_catalog.default.vendas_versioned (\n",
    "        id INT,\n",
    "        produto STRING,\n",
    "        quantidade INT,\n",
    "        preco DOUBLE,\n",
    "        data_venda DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inserção de Dados Iniciais (Snapshot 1)\n",
    "\n",
    "Vamos inserir os primeiros dados, criando o primeiro snapshot da tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_5tfxgxixKew"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserir dados iniciais\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas_versioned VALUES\n",
    "    (1, 'Produto A', 10, 100.0, DATE('2023-01-15')),\n",
    "    (2, 'Produto B', 5, 200.0, DATE('2023-01-16')),\n",
    "    (3, 'Produto C', 8, 150.0, DATE('2023-01-17'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando os Dados Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10|100.0|2023-01-15|\n",
      "|  2|Produto B|         5|200.0|2023-01-16|\n",
      "|  3|Produto C|         8|150.0|2023-01-17|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizar dados\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_versioned ORDER BY id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando o Primeiro Snapshot\n",
    "\n",
    "Cada operação no Iceberg cria um snapshot. Vamos ver o snapshot criado pela inserção inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+---------+\n",
      "|snapshot_id    |committed_at          |operation|\n",
      "+---------------+----------------------+---------+\n",
      "|513675073642359|2025-11-03 01:04:50.13|append   |\n",
      "+---------------+----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver snapshots da tabela\n",
    "spark.sql(\"SELECT snapshot_id, committed_at, operation FROM hadoop_catalog.default.vendas_versioned.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adicionando Mais Dados (Snapshot 2)\n",
    "\n",
    "Vamos adicionar mais dados, criando um segundo snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adicionar mais dados\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.vendas_versioned VALUES\n",
    "    (4, 'Produto D', 12, 300.0, DATE('2023-01-18')),\n",
    "    (5, 'Produto E', 7, 250.0, DATE('2023-01-19'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando Dados Após Segunda Inserção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10|100.0|2023-01-15|\n",
      "|  2|Produto B|         5|200.0|2023-01-16|\n",
      "|  3|Produto C|         8|150.0|2023-01-17|\n",
      "|  4|Produto D|        12|300.0|2023-01-18|\n",
      "|  5|Produto E|         7|250.0|2023-01-19|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizar dados atualizados\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_versioned ORDER BY id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando os Snapshots Existentes\n",
    "\n",
    "Agora devemos ter dois snapshots: um para cada operação de inserção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+---------+\n",
      "|snapshot_id        |committed_at          |operation|\n",
      "+-------------------+----------------------+---------+\n",
      "|513675073642359    |2025-11-03 01:04:50.13|append   |\n",
      "|2421557506823078562|2025-11-03 01:05:01.82|append   |\n",
      "+-------------------+----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver todos os snapshots\n",
    "spark.sql(\"SELECT snapshot_id, committed_at, operation FROM hadoop_catalog.default.vendas_versioned.snapshots ORDER BY committed_at\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Atualizando Dados (Snapshot 3)\n",
    "\n",
    "Vamos fazer uma atualização que pode ser considerada \"problemática\" e que queremos reverter depois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazer uma atualização \"problemática\"\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE hadoop_catalog.default.vendas_versioned \n",
    "    SET preco = preco * 10 \n",
    "    WHERE id <= 3\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizando Dados Após Atualização \"Problemática\"\n",
    "\n",
    "Os preços dos produtos 1, 2 e 3 foram multiplicados por 10 (erro simulado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+------+----------+\n",
      "| id|  produto|quantidade| preco|data_venda|\n",
      "+---+---------+----------+------+----------+\n",
      "|  1|Produto A|        10|1000.0|2023-01-15|\n",
      "|  2|Produto B|         5|2000.0|2023-01-16|\n",
      "|  3|Produto C|         8|1500.0|2023-01-17|\n",
      "|  4|Produto D|        12| 300.0|2023-01-18|\n",
      "|  5|Produto E|         7| 250.0|2023-01-19|\n",
      "+---+---------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver dados após atualização problemática\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_versioned ORDER BY id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando Todos os Snapshots\n",
    "\n",
    "Agora temos três snapshots: duas inserções e uma atualização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|513675073642359    |2025-11-03 01:04:50.13 |append   |\n",
      "|2421557506823078562|2025-11-03 01:05:01.82 |append   |\n",
      "|5583770746944098475|2025-11-03 01:05:13.069|overwrite|\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver todos os snapshots após update\n",
    "spark.sql(\"SELECT snapshot_id, committed_at, operation FROM hadoop_catalog.default.vendas_versioned.snapshots ORDER BY committed_at\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Travel - Consultando Snapshot Anterior\n",
    "\n",
    "Antes de fazer o rollback, vamos consultar como os dados estavam no snapshot anterior para confirmar que queremos voltar para lá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID do segundo snapshot: 2421557506823078562\n",
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10|100.0|2023-01-15|\n",
      "|  2|Produto B|         5|200.0|2023-01-16|\n",
      "|  3|Produto C|         8|150.0|2023-01-17|\n",
      "|  4|Produto D|        12|300.0|2023-01-18|\n",
      "|  5|Produto E|         7|250.0|2023-01-19|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consultar dados do snapshot anterior (antes da atualização problemática)\n",
    "# Primeiro, vamos pegar o ID do segundo snapshot\n",
    "snapshots = spark.sql(\"SELECT snapshot_id FROM hadoop_catalog.default.vendas_versioned.snapshots ORDER BY committed_at\")\n",
    "second_snapshot = snapshots.collect()[1][0]  # Segundo snapshot (índice 1)\n",
    "print(f\"ID do segundo snapshot: {second_snapshot}\")\n",
    "\n",
    "# Consultar dados como estavam no segundo snapshot\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM hadoop_catalog.default.vendas_versioned \n",
    "    VERSION AS OF {second_snapshot}\n",
    "    ORDER BY id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Executando Rollback\n",
    "\n",
    "Agora vamos fazer o rollback para o snapshot anterior, desfazendo a atualização problemática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[previous_snapshot_id: bigint, current_snapshot_id: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazer rollback para o segundo snapshot\n",
    "spark.sql(f\"\"\"\n",
    "    CALL hadoop_catalog.system.rollback_to_snapshot(\n",
    "        'hadoop_catalog.default.vendas_versioned', \n",
    "        {second_snapshot}\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando Dados Após Rollback\n",
    "\n",
    "Os dados devem estar como estavam antes da atualização problemática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10|100.0|2023-01-15|\n",
      "|  2|Produto B|         5|200.0|2023-01-16|\n",
      "|  3|Produto C|         8|150.0|2023-01-17|\n",
      "|  4|Produto D|        12|300.0|2023-01-18|\n",
      "|  5|Produto E|         7|250.0|2023-01-19|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar dados após rollback\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_versioned ORDER BY id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificando Snapshots Após Rollback\n",
    "\n",
    "O rollback cria um novo snapshot que aponta para o estado anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|513675073642359    |2025-11-03 01:04:50.13 |append   |\n",
      "|2421557506823078562|2025-11-03 01:05:01.82 |append   |\n",
      "|5583770746944098475|2025-11-03 01:05:13.069|overwrite|\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver snapshots após rollback\n",
    "spark.sql(\"SELECT snapshot_id, committed_at, operation FROM hadoop_catalog.default.vendas_versioned.snapshots ORDER BY committed_at\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Histórico Completo da Tabela\n",
    "\n",
    "Podemos ver o histórico completo de operações na tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2025-11-03 01:04:50.13 |513675073642359    |null               |true               |\n",
      "|2025-11-03 01:05:01.82 |2421557506823078562|513675073642359    |true               |\n",
      "|2025-11-03 01:05:13.069|5583770746944098475|2421557506823078562|false              |\n",
      "|2025-11-03 01:05:20.929|2421557506823078562|513675073642359    |true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver histórico completo da tabela\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_versioned.history ORDER BY made_current_at\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Rollback por Timestamp\n",
    "\n",
    "Além de rollback por snapshot ID, também podemos fazer rollback por timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollback por timestamp permite voltar a um ponto específico no tempo\n",
      "Útil quando você sabe quando o problema ocorreu, mas não o snapshot exato\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de rollback por timestamp (comentado para não afetar o exemplo)\n",
    "# from datetime import datetime, timedelta\n",
    "# \n",
    "# # Rollback para 10 minutos atrás\n",
    "# ten_minutes_ago = datetime.now() - timedelta(minutes=10)\n",
    "# timestamp_str = ten_minutes_ago.strftime('%Y-%m-%d %H:%M:%S')\n",
    "# \n",
    "# spark.sql(f\"\"\"\n",
    "#     CALL hadoop_catalog.system.rollback_to_timestamp(\n",
    "#         'hadoop_catalog.default.vendas_versioned', \n",
    "#         TIMESTAMP '{timestamp_str}'\n",
    "#     )\n",
    "# \"\"\")\n",
    "\n",
    "print(\"Rollback por timestamp permite voltar a um ponto específico no tempo\")\n",
    "print(\"Útil quando você sabe quando o problema ocorreu, mas não o snapshot exato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo dos Benefícios\n",
    "\n",
    "### Vantagens do Sistema de Rollback do Iceberg:\n",
    "\n",
    "1. **Operação Rápida**: Rollbacks são operações de metadados, não requerem reescrita de dados\n",
    "2. **Granularidade**: Pode voltar para qualquer snapshot ou timestamp específico\n",
    "3. **Segurança**: Dados antigos não são perdidos, apenas \"escondidos\"\n",
    "4. **Auditoria**: Histórico completo de todas as operações\n",
    "5. **Recuperação**: Facilita recuperação de erros humanos ou de sistema\n",
    "\n",
    "### Casos de Uso Comuns:\n",
    "\n",
    "- Desfazer atualizações incorretas\n",
    "- Recuperar de corrupção de dados\n",
    "- Voltar a um estado conhecido bom\n",
    "- Testes e desenvolvimento\n",
    "- Compliance e auditoria"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
