{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integração Apache Iceberg com Bancos Relacionais\n",
    "\n",
    "Este notebook demonstra como integrar dados do Apache Iceberg com bancos relacionais (PostgreSQL), realizando operações de merge e sincronização de dados entre diferentes sistemas de armazenamento.\n",
    "\n",
    "## Contexto e Objetivos:\n",
    "\n",
    "### Cenário Empresarial:\n",
    "- **Sistemas OLTP**: PostgreSQL com dados transacionais em tempo real\n",
    "- **Data Lake**: Apache Iceberg para análises e histórico\n",
    "- **Necessidade**: Sincronização eficiente entre os sistemas\n",
    "- **Desafio**: Manter consistência e performance\n",
    "\n",
    "### Benefícios da Integração:\n",
    "- **Arquitetura Híbrida**: Melhor de ambos os mundos (OLTP + OLAP)\n",
    "- **Análises em Tempo Real**: Dados atualizados para BI\n",
    "- **Histórico Completo**: Versionamento e auditoria no Iceberg\n",
    "- **Performance**: Otimizações específicas para cada workload\n",
    "- **Escalabilidade**: Separação de cargas transacionais e analíticas\n",
    "\n",
    "### Estratégias de Sincronização:\n",
    "1. **Batch ETL**: Cargas periódicas completas ou incrementais\n",
    "2. **Change Data Capture (CDC)**: Captura de mudanças em tempo real\n",
    "3. **Merge Operations**: Upserts eficientes com MERGE INTO\n",
    "4. **Event-Driven**: Sincronização baseada em eventos\n",
    "\n",
    "## Setup do Ambiente\n",
    "\n",
    "Configuração do Spark com suporte ao Iceberg e driver JDBC para PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HUDWop-AioaE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warehouse configurado para: file:///home/tavares/warehouse\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "# Sessão Spark\n",
    "# Para o Spark se estiver rodando\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergRollbacks\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", \"file:///home/tavares/warehouse\") \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Warehouse configurado para: {spark.conf.get('spark.sql.catalog.hadoop_catalog.warehouse')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações para Manipulação de Datas\n",
    "\n",
    "Bibliotecas necessárias para trabalhar com datas e períodos, úteis para operações de sincronização incremental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação da Tabela Iceberg de Destino\n",
    "\n",
    "Criamos uma tabela Iceberg que espelha a estrutura da tabela `customers` do PostgreSQL. Esta tabela servirá como destino para os dados sincronizados e aproveitará todos os benefícios do Iceberg como versionamento, ACID transactions e schema evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criamos a tabela customers no Iceberg\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.customers\")\n",
    "\n",
    "''' Este formato de tabela apresentou erros nos scripts abaixo. Com isso foi retirado o NOT NULL dos dois primeiros campos.\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE hadoop_catalog.default.customers (\n",
    "        customer_id string NOT NULL,\n",
    "        company_name string NOT NULL,\n",
    "        contact_name string,\n",
    "        contact_title string,\n",
    "        address string,\n",
    "        city string,\n",
    "        region string,\n",
    "        postal_code string,\n",
    "        country string,\n",
    "        phone string,\n",
    "        fax string\n",
    "    )\n",
    "    USING iceberg          \n",
    "\"\"\")\n",
    "'''\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE hadoop_catalog.default.customers (\n",
    "        customer_id string,\n",
    "        company_name string,\n",
    "        contact_name string,\n",
    "        contact_title string,\n",
    "        address string,\n",
    "        city string,\n",
    "        region string,\n",
    "        postal_code string,\n",
    "        country string,\n",
    "        phone string,\n",
    "        fax string\n",
    "    )\n",
    "    USING iceberg          \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração da Conexão JDBC\n",
    "\n",
    "Definimos os parâmetros de conexão com o banco PostgreSQL. O hostname `postgres-erp` corresponde ao nome do serviço definido no docker-compose, permitindo comunicação entre containers na mesma rede Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credenciais do PostgreSQL\n",
    "jdbc_hostname = \"postgres-erp\"\n",
    "jdbc_port = 5432\n",
    "jdbc_database = \"northwind\"\n",
    "jdbc_username = \"postgres\"\n",
    "jdbc_password = \"postgres\"\n",
    "\n",
    "# URL JDBC de conexão\n",
    "jdbc_url = f\"jdbc:postgresql://{jdbc_hostname}:{jdbc_port}/{jdbc_database}\"\n",
    "\n",
    "# Propriedades de conexão JDBC\n",
    "connection_properties = {\n",
    "    \"user\": jdbc_username,\n",
    "    \"password\": jdbc_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura de Dados do PostgreSQL\n",
    "\n",
    "Implementamos uma função para ler dados da tabela `customers` do PostgreSQL usando JDBC. A query é encapsulada em uma subquery para permitir maior flexibilidade na seleção de dados. Esta abordagem permite:\n",
    "\n",
    "- **Filtragem**: Aplicar WHERE clauses para dados específicos\n",
    "- **Transformações**: Aplicar funções SQL durante a leitura\n",
    "- **Performance**: Reduzir volume de dados transferidos\n",
    "- **Flexibilidade**: Adaptar queries conforme necessário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------------+--------------------+--------------------+------------+------+-----------+-----------+--------------+--------------+\n",
      "|customer_id|        company_name|      contact_name|       contact_title|             address|        city|region|postal_code|    country|         phone|           fax|\n",
      "+-----------+--------------------+------------------+--------------------+--------------------+------------+------+-----------+-----------+--------------+--------------+\n",
      "|      ALFKI| Alfreds Futterkiste|      Maria Anders|Sales Representative|       Obere Str. 57|      Berlin|  null|      12209|    Germany|   030-0074321|   030-0076545|\n",
      "|      ANATR|Ana Trujillo Empa...|      Ana Trujillo|               Owner|Avda. de la Const...| México D.F.|  null|      05021|     Mexico|  (5) 555-4729|  (5) 555-3745|\n",
      "|      ANTON|Antonio Moreno Ta...|    Antonio Moreno|               Owner|     Mataderos  2312| México D.F.|  null|      05023|     Mexico|  (5) 555-3932|          null|\n",
      "|      AROUT|     Around the Horn|      Thomas Hardy|Sales Representative|     120 Hanover Sq.|      London|  null|    WA1 1DP|         UK|(171) 555-7788|(171) 555-6750|\n",
      "|      BERGS|  Berglunds snabbköp|Christina Berglund| Order Administrator|     Berguvsvägen  8|       Luleå|  null|   S-958 22|     Sweden| 0921-12 34 65| 0921-12 34 67|\n",
      "|      BLAUS|Blauer See Delika...|        Hanna Moos|Sales Representative|      Forsterstr. 57|    Mannheim|  null|      68306|    Germany|    0621-08460|    0621-08924|\n",
      "|      BLONP|Blondesddsl père ...|Frédérique Citeaux|   Marketing Manager|    24, place Kléber|  Strasbourg|  null|      67000|     France|   88.60.15.31|   88.60.15.32|\n",
      "|      BOLID|Bólido Comidas pr...|     Martín Sommer|               Owner|      C/ Araquil, 67|      Madrid|  null|      28023|      Spain|(91) 555 22 82|(91) 555 91 99|\n",
      "|      BONAP|            Bon app'|  Laurence Lebihan|               Owner|12, rue des Bouchers|   Marseille|  null|      13008|     France|   91.24.45.40|   91.24.45.41|\n",
      "|      BOTTM|Bottom-Dollar Mar...| Elizabeth Lincoln|  Accounting Manager|  23 Tsawassen Blvd.|   Tsawassen|    BC|    T2F 8M4|     Canada|(604) 555-4729|(604) 555-3745|\n",
      "|      BSBEV|       B's Beverages| Victoria Ashworth|Sales Representative|   Fauntleroy Circus|      London|  null|    EC2 5NT|         UK|(171) 555-1212|          null|\n",
      "|      CACTU|Cactus Comidas pa...|  Patricio Simpson|         Sales Agent|         Cerrito 333|Buenos Aires|  null|       1010|  Argentina|  (1) 135-5555|  (1) 135-4892|\n",
      "|      CENTC|Centro comercial ...|   Francisco Chang|   Marketing Manager|Sierras de Granad...| México D.F.|  null|      05022|     Mexico|  (5) 555-3392|  (5) 555-7293|\n",
      "|      CHOPS|   Chop-suey Chinese|         Yang Wang|               Owner|        Hauptstr. 29|        Bern|  null|       3012|Switzerland|   0452-076545|          null|\n",
      "|      COMMI|    Comércio Mineiro|      Pedro Afonso|     Sales Associate|Av. dos Lusíadas, 23|   Sao Paulo|    SP|  05432-043|     Brazil| (11) 555-7647|          null|\n",
      "|      CONSH|Consolidated Hold...|   Elizabeth Brown|Sales Representative|Berkeley Gardens ...|      London|  null|    WX1 6LT|         UK|(171) 555-2282|(171) 555-9199|\n",
      "|      DRACD|Drachenblut Delik...|      Sven Ottlieb| Order Administrator|        Walserweg 21|      Aachen|  null|      52066|    Germany|   0241-039123|   0241-059428|\n",
      "|      DUMON|     Du monde entier|    Janine Labrune|               Owner|67, rue des Cinqu...|      Nantes|  null|      44000|     France|   40.67.88.88|   40.67.89.89|\n",
      "|      EASTC|  Eastern Connection|         Ann Devon|         Sales Agent|      35 King George|      London|  null|    WX3 6FW|         UK|(171) 555-0297|(171) 555-3373|\n",
      "|      ERNSH|        Ernst Handel|     Roland Mendel|       Sales Manager|        Kirchgasse 6|        Graz|  null|       8010|    Austria|     7675-3425|     7675-3426|\n",
      "+-----------+--------------------+------------------+--------------------+--------------------+------------+------+-----------+-----------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_postgres_data():\n",
    "    query = \"\"\"\n",
    "        (SELECT *\n",
    "         FROM customers\n",
    "        ) AS customers\n",
    "    \"\"\"\n",
    "    df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=query,\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Ler dados do PostgreSQL\n",
    "df_postgres = read_postgres_data()\n",
    "\n",
    "# Exibir os dados lidos\n",
    "df_postgres.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento Inicial dos Dados\n",
    "\n",
    "Realizamos o carregamento inicial (full load) dos dados do PostgreSQL para a tabela Iceberg. Esta operação cria o primeiro snapshot da tabela e estabelece a base para futuras sincronizações incrementais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados na tabela Iceberg\n",
      "+-----+\n",
      "|total|\n",
      "+-----+\n",
      "|   91|\n",
      "+-----+\n",
      "\n",
      "+-----------+--------------------+------------------+--------------------+--------------------+-----------+------+-----------+-------+--------------+--------------+\n",
      "|customer_id|        company_name|      contact_name|       contact_title|             address|       city|region|postal_code|country|         phone|           fax|\n",
      "+-----------+--------------------+------------------+--------------------+--------------------+-----------+------+-----------+-------+--------------+--------------+\n",
      "|      ALFKI| Alfreds Futterkiste|      Maria Anders|Sales Representative|       Obere Str. 57|     Berlin|  null|      12209|Germany|   030-0074321|   030-0076545|\n",
      "|      ANATR|Ana Trujillo Empa...|      Ana Trujillo|               Owner|Avda. de la Const...|México D.F.|  null|      05021| Mexico|  (5) 555-4729|  (5) 555-3745|\n",
      "|      ANTON|Antonio Moreno Ta...|    Antonio Moreno|               Owner|     Mataderos  2312|México D.F.|  null|      05023| Mexico|  (5) 555-3932|          null|\n",
      "|      AROUT|     Around the Horn|      Thomas Hardy|Sales Representative|     120 Hanover Sq.|     London|  null|    WA1 1DP|     UK|(171) 555-7788|(171) 555-6750|\n",
      "|      BERGS|  Berglunds snabbköp|Christina Berglund| Order Administrator|     Berguvsvägen  8|      Luleå|  null|   S-958 22| Sweden| 0921-12 34 65| 0921-12 34 67|\n",
      "+-----------+--------------------+------------------+--------------------+--------------------+-----------+------+-----------+-------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados do PostgreSQL para Iceberg\n",
    "df_postgres.writeTo(\"hadoop_catalog.default.customers\").append()\n",
    "\n",
    "print(\"Dados carregados na tabela Iceberg\")\n",
    "\n",
    "# Verificar dados na tabela Iceberg\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM hadoop_catalog.default.customers\").show()\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.customers LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulação de Mudanças nos Dados\n",
    "\n",
    "Para demonstrar o processo de merge, vamos simular mudanças que normalmente ocorreriam no sistema PostgreSQL:\n",
    "\n",
    "- **INSERT**: Novos clientes adicionados\n",
    "- **UPDATE**: Informações de clientes existentes modificadas\n",
    "- **DELETE**: Clientes removidos (se aplicável)\n",
    "\n",
    "Em um ambiente real, essas mudanças seriam capturadas através de CDC ou queries incrementais baseadas em timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mudanças simuladas aplicadas\n",
      "+-----+\n",
      "|total|\n",
      "+-----+\n",
      "|   92|\n",
      "+-----+\n",
      "\n",
      "+-----------+-------------------+-------------+--------------------+-------------+--------+------+-----------+-------+--------+-----------+\n",
      "|customer_id|       company_name| contact_name|       contact_title|      address|    city|region|postal_code|country|   phone|        fax|\n",
      "+-----------+-------------------+-------------+--------------------+-------------+--------+------+-----------+-------+--------+-----------+\n",
      "|      ALFKI|Alfreds Futterkiste|Maria Updated|Sales Representative|Obere Str. 57|  Berlin|  null|      12209|Germany|555-9999|030-0076545|\n",
      "|      NEWCO|    New Company Ltd|     John Doe|             Manager|   123 New St|New City|    NY|      12345|    USA|555-1234|   555-5678|\n",
      "+-----------+-------------------+-------------+--------------------+-------------+--------+------+-----------+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simular mudanças adicionando novos dados diretamente na tabela Iceberg\n",
    "# (Em um cenário real, essas mudanças viriam do PostgreSQL)\n",
    "\n",
    "# Adicionar um novo cliente\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO hadoop_catalog.default.customers VALUES\n",
    "    ('NEWCO', 'New Company Ltd', 'John Doe', 'Manager', '123 New St', 'New City', 'NY', '12345', 'USA', '555-1234', '555-5678')\n",
    "\"\"\")\n",
    "\n",
    "# Atualizar um cliente existente\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE hadoop_catalog.default.customers \n",
    "    SET phone = '555-9999', contact_name = 'Maria Updated'\n",
    "    WHERE customer_id = 'ALFKI'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Mudanças simuladas aplicadas\")\n",
    "\n",
    "# Verificar as mudanças\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM hadoop_catalog.default.customers\").show()\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.customers WHERE customer_id IN ('NEWCO', 'ALFKI')\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operação MERGE INTO\n",
    "\n",
    "O `MERGE INTO` é uma operação fundamental para sincronização de dados, permitindo:\n",
    "\n",
    "- **UPSERT**: Inserir novos registros ou atualizar existentes em uma única operação\n",
    "- **Atomicidade**: Operação ACID que garante consistência\n",
    "- **Performance**: Mais eficiente que INSERT + UPDATE separados\n",
    "- **Flexibilidade**: Diferentes ações baseadas em condições\n",
    "\n",
    "Esta operação é essencial para manter a sincronização entre PostgreSQL e Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGE INTO executado com sucesso\n",
      "+-----------+---------------------------+------------+-------------+---------------+------------+------+-----------+-------+-----------+-----------+\n",
      "|customer_id|company_name               |contact_name|contact_title|address        |city        |region|postal_code|country|phone      |fax        |\n",
      "+-----------+---------------------------+------------+-------------+---------------+------------+------+-----------+-------+-----------+-----------+\n",
      "|ALFKI      |Alfreds Futterkiste UPDATED|Maria Final |Sales Rep    |New Address 123|Berlin      |null  |12209      |Germany|030-0074321|030-0076545|\n",
      "|NEWC2      |Another New Company        |Jane Smith  |Director     |456 Another St |Another City|CA    |54321      |USA    |555-4321   |555-8765   |\n",
      "+-----------+---------------------------+------------+-------------+---------------+------------+------+-----------+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criar uma tabela temporária com dados \"novos\" do PostgreSQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW customers_updates AS\n",
    "    SELECT 'ALFKI' as customer_id, 'Alfreds Futterkiste UPDATED' as company_name, 'Maria Final' as contact_name, \n",
    "           'Sales Rep' as contact_title, 'New Address 123' as address, 'Berlin' as city, \n",
    "           NULL as region, '12209' as postal_code, 'Germany' as country, \n",
    "           '030-0074321' as phone, '030-0076545' as fax\n",
    "    UNION ALL\n",
    "    SELECT 'NEWC2' as customer_id, 'Another New Company' as company_name, 'Jane Smith' as contact_name,\n",
    "           'Director' as contact_title, '456 Another St' as address, 'Another City' as city,\n",
    "           'CA' as region, '54321' as postal_code, 'USA' as country,\n",
    "           '555-4321' as phone, '555-8765' as fax\n",
    "\"\"\")\n",
    "\n",
    "# Executar MERGE INTO\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO hadoop_catalog.default.customers AS target\n",
    "    USING customers_updates AS source\n",
    "    ON target.customer_id = source.customer_id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET \n",
    "            company_name = source.company_name,\n",
    "            contact_name = source.contact_name,\n",
    "            contact_title = source.contact_title,\n",
    "            address = source.address,\n",
    "            phone = source.phone\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (customer_id, company_name, contact_name, contact_title, address, city, region, postal_code, country, phone, fax)\n",
    "        VALUES (source.customer_id, source.company_name, source.contact_name, source.contact_title, \n",
    "                source.address, source.city, source.region, source.postal_code, source.country, source.phone, source.fax)\n",
    "\"\"\")\n",
    "\n",
    "print(\"MERGE INTO executado com sucesso\")\n",
    "\n",
    "# Verificar resultados\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.customers WHERE customer_id IN ('ALFKI', 'NEWC2')\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoramento e Auditoria\n",
    "\n",
    "Uma das grandes vantagens do Iceberg é o sistema de snapshots que permite auditoria completa de todas as operações. Cada operação (INSERT, UPDATE, DELETE, MERGE) cria um novo snapshot, proporcionando:\n",
    "\n",
    "- **Rastreabilidade**: Histórico completo de mudanças\n",
    "- **Rollback**: Capacidade de reverter para estados anteriores\n",
    "- **Time Travel**: Consultas em pontos específicos do tempo\n",
    "- **Compliance**: Atendimento a requisitos de auditoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histórico de Snapshots:\n",
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|894653696084846250 |2025-11-06 23:55:01.399|append   |\n",
      "|3151425052900505849|2025-11-06 23:55:09.852|append   |\n",
      "|5501151205264156785|2025-11-06 23:55:14.978|overwrite|\n",
      "|1516900522667827493|2025-11-06 23:55:22.347|overwrite|\n",
      "+-------------------+-----------------------+---------+\n",
      "\n",
      "\n",
      "Histórico da Tabela:\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2025-11-06 23:55:01.399|894653696084846250 |null               |true               |\n",
      "|2025-11-06 23:55:09.852|3151425052900505849|894653696084846250 |true               |\n",
      "|2025-11-06 23:55:14.978|5501151205264156785|3151425052900505849|true               |\n",
      "|2025-11-06 23:55:22.347|1516900522667827493|5501151205264156785|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "Contagem final de registros:\n",
      "+-----+\n",
      "|total|\n",
      "+-----+\n",
      "|   93|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar snapshots após todas as operações\n",
    "print(\"Histórico de Snapshots:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT snapshot_id, committed_at, operation\n",
    "    FROM hadoop_catalog.default.customers.snapshots \n",
    "    ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Verificar histórico completo\n",
    "print(\"\\nHistórico da Tabela:\")\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.customers.history\").show(truncate=False)\n",
    "\n",
    "# Contagem final\n",
    "print(\"\\nContagem final de registros:\")\n",
    "spark.sql(\"SELECT COUNT(*) as total FROM hadoop_catalog.default.customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo dos Benefícios e Casos de Uso\n",
    "\n",
    "### Vantagens da Integração Iceberg + PostgreSQL:\n",
    "\n",
    "1. **Arquitetura Híbrida Otimizada**:\n",
    "   - PostgreSQL: Transações OLTP de alta performance\n",
    "   - Iceberg: Análises OLAP escaláveis e eficientes\n",
    "\n",
    "2. **Operações ACID Completas**:\n",
    "   - Consistência garantida em operações de merge\n",
    "   - Isolamento entre leituras e escritas\n",
    "   - Durabilidade dos dados\n",
    "\n",
    "3. **Versionamento e Auditoria**:\n",
    "   - Histórico completo de mudanças\n",
    "   - Capacidade de rollback\n",
    "   - Compliance regulatório\n",
    "\n",
    "4. **Performance Otimizada**:\n",
    "   - Particionamento inteligente no Iceberg\n",
    "   - Compactação automática de arquivos\n",
    "   - Índices e estatísticas para otimização de queries\n",
    "\n",
    "5. **Flexibilidade Operacional**:\n",
    "   - Schema evolution sem downtime\n",
    "   - Múltiplas estratégias de sincronização\n",
    "   - Integração com ferramentas de BI\n",
    "\n",
    "### Casos de Uso Empresariais:\n",
    "\n",
    "- **Data Warehousing**: ETL de sistemas transacionais para análise\n",
    "- **Real-time Analytics**: Dashboards com dados atualizados\n",
    "- **Data Lake Architecture**: Centralização de dados de múltiplas fontes\n",
    "- **Backup e Arquivamento**: Histórico de dados transacionais\n",
    "- **Compliance**: Auditoria e rastreabilidade de mudanças\n",
    "- **Machine Learning**: Datasets para treinamento de modelos\n",
    "\n",
    "### Considerações para Produção:\n",
    "\n",
    "- **Monitoramento**: Acompanhar performance e latência\n",
    "- **Segurança**: Criptografia e controle de acesso\n",
    "- **Backup**: Estratégias de recuperação de desastres\n",
    "- **Escalabilidade**: Planejamento para crescimento de dados\n",
    "- **Manutenção**: Compactação e limpeza de snapshots antigos"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMhOnuNNzAcBD330bu+rsmn",
   "provenance": [
    {
     "file_id": "178ZW6B8jLzISq4ceO5QdkIi2ZG-ybMAp",
     "timestamp": 1730935056622
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
